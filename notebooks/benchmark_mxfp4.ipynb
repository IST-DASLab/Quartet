{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=8\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import time\n",
    "\n",
    "from fast_hadamard_transform import hadamard_transform\n",
    "\n",
    "\n",
    "DTYPE = torch.bfloat16\n",
    "# DTYPE = torch.float32\n",
    "GRID = torch.tensor(\n",
    "    [-6.0, -4.0, -3.0, -2.0, -1.5, -1.0, -0.5, 0.0,\n",
    "    0.0,  0.5,  1.0,  1.5,  2.0,  3.0,  4.0, 6.0],\n",
    "    device=\"cuda\", dtype=DTYPE,\n",
    ")\n",
    "EMAX = 2\n",
    "SCALE = 3/4\n",
    "GAUSSIAL_SCALE = 2.92247856 / 6.0\n",
    "\n",
    "\n",
    "### FORWARD QUANTIZATION\n",
    "\n",
    "def rtn_fp4(x, grid):\n",
    "    inds = torch.bucketize(x, grid)\n",
    "\n",
    "    lo = torch.clamp(inds - 1, min=0, max=15)\n",
    "    hi = torch.clamp(inds,     min=0, max=15)\n",
    "\n",
    "    g_lo = grid[lo]\n",
    "    g_hi = grid[hi]\n",
    "\n",
    "    pick_hi = (g_hi - x) <= (x - g_lo)\n",
    "    return torch.where(pick_hi, g_hi, g_lo)\n",
    "\n",
    "\n",
    "@torch.compile\n",
    "def quantize_quest(x):\n",
    "    x_grouped = x.view(-1, 32)\n",
    "    shared_exps = torch.floor(torch.log2(\n",
    "        GAUSSIAL_SCALE * torch.std(x_grouped, dim=-1, correction=0, keepdim=True) + 1e-8\n",
    "    ))\n",
    "    scales = 2 ** shared_exps\n",
    "    \n",
    "    scaled_x = x_grouped / scales\n",
    "    \n",
    "    x_fp4 = rtn_fp4(scaled_x, GRID)\n",
    "    \n",
    "    return (x_fp4 * scales).reshape_as(x), torch.abs(scaled_x) <= 6.0\n",
    "\n",
    "\n",
    "### BACKWARD QUANTIZATION\n",
    "\n",
    "def stochastic_round_fp4(x, grid):\n",
    "    inds = torch.bucketize(x, grid)  \n",
    "    \n",
    "    lo = torch.clamp(inds - 1, min=0, max=15)  \n",
    "    hi = torch.clamp(inds,     min=0, max=15)  \n",
    "\n",
    "    g_lo = grid[lo]\n",
    "    g_hi = grid[hi]\n",
    "\n",
    "    delta = g_hi - g_lo\n",
    "    p = torch.where(\n",
    "        delta > 0,\n",
    "        (x - g_lo) / delta,\n",
    "        torch.full_like(x, 0.5)\n",
    "    )\n",
    "\n",
    "    u = torch.rand_like(x)\n",
    "    pick_hi = u < p\n",
    "    return torch.where(pick_hi, g_hi, g_lo)\n",
    "\n",
    "\n",
    "@torch.compile\n",
    "def quantize_tseng(x):\n",
    "    x_grouped = x.view(-1, 32)\n",
    "    shared_exps = torch.floor(torch.log2(x_grouped.abs().max(dim=-1, keepdim=True)[0])) - EMAX\n",
    "    scales = 2 ** shared_exps / SCALE\n",
    "    \n",
    "    scaled_x = x_grouped / scales\n",
    "\n",
    "    # x_fp4 = stochastic_round_fp4(scaled_x, GRID)\n",
    "    x_fp4 = rtn_fp4(scaled_x, GRID)\n",
    "\n",
    "    return (x_fp4 * scales).reshape_as(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HadamardGemm(torch.autograd.Function):\n",
    "    forward_hadamard_matrix = hadamard_transform(torch.eye(32, dtype=DTYPE, device=\"cuda\"), scale=32**(-1/2))\n",
    "    backward_hadamard_matrix = hadamard_transform(torch.eye(32, dtype=DTYPE, device=\"cuda\"), scale=32**(-1/2))\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, input, weight, deterministic=False):\n",
    "        ctx.batch = input.shape[0]\n",
    "        ctx.seq = input.shape[1]\n",
    "        ctx.in_dim = weight.shape[1]\n",
    "        ctx.out_dim = weight.shape[0]\n",
    "        \n",
    "        ctx.deterministic = deterministic\n",
    "        \n",
    "        input_hf = (\n",
    "            input.reshape(-1, 32) @ HadamardGemm.forward_hadamard_matrix\n",
    "        ).view(ctx.batch, ctx.seq, ctx.in_dim)\n",
    "        weight_hf = (\n",
    "            weight.reshape(-1, 32) @ HadamardGemm.forward_hadamard_matrix\n",
    "        ).view(ctx.out_dim, ctx.in_dim)\n",
    "        \n",
    "        ctx.save_for_backward(input_hf, weight_hf)\n",
    "        return F.linear(input_hf, weight_hf)\n",
    "    \n",
    "    @staticmethod\n",
    "    @torch.compile()\n",
    "    def backward(ctx, grad_output):\n",
    "        input_hf, weight_hf = ctx.saved_tensors\n",
    "        \n",
    "        if not ctx.deterministic:\n",
    "            HadamardGemm.backward_hadamard_matrix = HadamardGemm.backward_hadamard_matrix @ torch.diag(\n",
    "                torch.randint(\n",
    "                    0, 2, (32,),\n",
    "                    device=HadamardGemm.backward_hadamard_matrix.device,\n",
    "                    dtype=HadamardGemm.backward_hadamard_matrix.dtype\n",
    "                ) * 2 - 1\n",
    "            )\n",
    "        \n",
    "        grad_output_hb = (\n",
    "            grad_output.reshape(-1, 32) @ HadamardGemm.backward_hadamard_matrix\n",
    "        ).view(ctx.batch, ctx.seq, ctx.out_dim)\n",
    "        hft_weightt_hb = (\n",
    "            weight_hf.T.reshape(-1, 32) @ HadamardGemm.backward_hadamard_matrix\n",
    "        ).view(ctx.in_dim, ctx.out_dim)\n",
    "        grad_input_hf = F.linear(grad_output_hb, hft_weightt_hb)\n",
    "        grad_input = (\n",
    "            grad_input_hf.reshape(-1, 32) @ HadamardGemm.forward_hadamard_matrix.T\n",
    "        ).view(ctx.batch, ctx.seq, ctx.in_dim)\n",
    "        \n",
    "        grad_outputt_hb = (\n",
    "            grad_output.view(-1, ctx.out_dim).T.reshape(-1, 32) @ HadamardGemm.backward_hadamard_matrix\n",
    "        ).view(ctx.out_dim, -1)\n",
    "        hft_inputt_hb = (\n",
    "            input_hf.view(-1, ctx.in_dim).T.reshape(-1, 32) @ HadamardGemm.backward_hadamard_matrix\n",
    "        ).view(ctx.in_dim, -1)\n",
    "        grad_weight_hf = F.linear(grad_outputt_hb, hft_inputt_hb)\n",
    "        grad_weight = (\n",
    "            grad_weight_hf.reshape(-1, 32) @ HadamardGemm.forward_hadamard_matrix.T\n",
    "        ).view(ctx.out_dim, ctx.in_dim)\n",
    "        return grad_input, grad_weight, None\n",
    "\n",
    "\n",
    "class MXFP4Gemm(torch.autograd.Function):\n",
    "    forward_hadamard_matrix = hadamard_transform(torch.eye(32, dtype=DTYPE, device=\"cuda\"), scale=32**(-1/2))\n",
    "    backward_hadamard_matrix = hadamard_transform(torch.eye(32, dtype=DTYPE, device=\"cuda\"), scale=32**(-1/2))\n",
    "    \n",
    "    def forward(ctx, input, weight, deterministic=False):\n",
    "        ctx.batch = input.shape[0]\n",
    "        ctx.seq = input.shape[1]\n",
    "        ctx.in_dim = weight.shape[1]\n",
    "        ctx.out_dim = weight.shape[0]\n",
    "        \n",
    "        ctx.deterministic = deterministic\n",
    "        \n",
    "        input_hf, input_mask_hf = quantize_quest(\n",
    "            input.view(-1, 32) @ MXFP4Gemm.forward_hadamard_matrix\n",
    "        )\n",
    "        input_hf, input_mask_hf = input_hf.view_as(input), input_mask_hf.view_as(input)\n",
    "        weight_hf, weight_mask_hf = quantize_quest(\n",
    "            weight.view(-1, 32) @ MXFP4Gemm.forward_hadamard_matrix\n",
    "        )\n",
    "        weight_hf, weight_mask_hf = weight_hf.view_as(weight), weight_mask_hf.view_as(weight)\n",
    "        \n",
    "        ctx.save_for_backward(input_hf, weight_hf, input_mask_hf, weight_mask_hf)\n",
    "        return F.linear(input_hf, weight_hf)\n",
    "    \n",
    "    @torch.compile()\n",
    "    def backward(ctx, grad_output):\n",
    "        input_hf, weight_hf, input_mask_hf, weight_mask_hf = ctx.saved_tensors\n",
    "        \n",
    "        if not ctx.deterministic:\n",
    "            MXFP4Gemm.backward_hadamard_matrix = MXFP4Gemm.backward_hadamard_matrix @ torch.diag(\n",
    "                torch.randint(\n",
    "                    0, 2, (32,),\n",
    "                    device=MXFP4Gemm.backward_hadamard_matrix.device,\n",
    "                    dtype=MXFP4Gemm.backward_hadamard_matrix.dtype\n",
    "                ) * 2 - 1\n",
    "            )\n",
    "        \n",
    "        grad_output_hb = quantize_tseng(\n",
    "            grad_output.view(-1, 32) @ MXFP4Gemm.backward_hadamard_matrix.T\n",
    "        ).view(ctx.batch, ctx.seq, ctx.out_dim)\n",
    "        hft_weightt_hb = quantize_tseng(\n",
    "            weight_hf.T.reshape(-1, 32) @ MXFP4Gemm.backward_hadamard_matrix\n",
    "        ).view(ctx.in_dim, ctx.out_dim)\n",
    "        grad_input_hf = F.linear(grad_output_hb, hft_weightt_hb)\n",
    "        grad_input = (\n",
    "            (grad_input_hf.view(-1, 32) * input_mask_hf.view(-1, 32).to(grad_input_hf.dtype))\n",
    "            @ MXFP4Gemm.forward_hadamard_matrix.T\n",
    "        ).view(ctx.batch, ctx.seq, ctx.in_dim)\n",
    "        \n",
    "        grad_outputt_hb = quantize_tseng(\n",
    "            grad_output.view(-1, ctx.out_dim).T.reshape(-1, 32) @ MXFP4Gemm.backward_hadamard_matrix\n",
    "        ).view(ctx.out_dim, -1)\n",
    "        hft_inputt_hb = quantize_tseng(\n",
    "            input_hf.view(-1, ctx.in_dim).T.reshape(-1, 32) @ MXFP4Gemm.backward_hadamard_matrix\n",
    "        ).view(ctx.in_dim, -1)\n",
    "        grad_weight_hf = F.linear(grad_outputt_hb, hft_inputt_hb)\n",
    "        grad_weight = (\n",
    "            (grad_weight_hf.view(-1, 32) * weight_mask_hf.view(-1, 32).to(grad_weight_hf.dtype))\n",
    "            @ MXFP4Gemm.forward_hadamard_matrix.T\n",
    "        ).view(ctx.out_dim, ctx.in_dim)\n",
    "        return grad_input, grad_weight, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "\n",
    "import torch\n",
    "import triton\n",
    "import triton.language as tl\n",
    "from fast_hadamard_transform import hadamard_transform\n",
    "\n",
    "\n",
    "@triton.autotune(\n",
    "    configs=[\n",
    "        triton.Config({\"BLOCK_SIZE\": 32 * 32}),\n",
    "        triton.Config({\"BLOCK_SIZE\": 64 * 32}),\n",
    "        triton.Config({\"BLOCK_SIZE\": 128 * 32}),\n",
    "        triton.Config({\"BLOCK_SIZE\": 256 * 32}),\n",
    "        triton.Config({\"BLOCK_SIZE\": 512 * 32}),\n",
    "    ],\n",
    "    key=[],\n",
    ")\n",
    "@triton.jit\n",
    "def mxfp4_forward_kernel(\n",
    "    x_ptr,\n",
    "    hadamard_matrix_ptr,\n",
    "    output_ptr,\n",
    "    clip_mask_ptr,\n",
    "    n_elements: tl.constexpr,\n",
    "    hadamard_dim: tl.constexpr,\n",
    "    group_size: tl.constexpr,\n",
    "    seed: int,\n",
    "    quest: tl.constexpr,\n",
    "    stochastic_round: tl.constexpr,\n",
    "    BLOCK_SIZE: tl.constexpr,\n",
    "):    \n",
    "    offsets_hadamard = tl.arange(0, hadamard_dim * hadamard_dim)\n",
    "    hadamard_matrix = tl.load(hadamard_matrix_ptr + offsets_hadamard).reshape(hadamard_dim, hadamard_dim)\n",
    "    \n",
    "    # load x\n",
    "    pid = tl.program_id(0)\n",
    "    start_idx = pid * BLOCK_SIZE\n",
    "    offsets = start_idx + tl.arange(0, BLOCK_SIZE)\n",
    "    mask = offsets < n_elements\n",
    "    x_flat = tl.load(x_ptr + offsets, mask=mask)\n",
    "    \n",
    "    # hadamard transform\n",
    "    x = tl.reshape(x_flat, (BLOCK_SIZE // hadamard_dim, hadamard_dim))\n",
    "    x_had = tl.dot(x, hadamard_matrix)\n",
    "    \n",
    "    # group\n",
    "    x_had_grouped = tl.reshape(x_had, (BLOCK_SIZE // group_size, group_size))\n",
    "    \n",
    "    # scale\n",
    "    if quest:\n",
    "        mean_squared = tl.sum(x_had_grouped * x_had_grouped, axis=-1, keep_dims=True) / group_size\n",
    "        mean = tl.sum(x_had_grouped, axis=-1, keep_dims=True) / group_size\n",
    "        std = tl.sqrt(mean_squared - mean * mean)\n",
    "        scales = (2.92247856 / 6.0) * std + 1e-8\n",
    "        shared_exps = tl.exp2(tl.floor(tl.log2(scales)))\n",
    "        x_had_scaled = x_had_grouped / shared_exps\n",
    "    else:\n",
    "        scales = tl.max(tl.abs(x_had_grouped), axis=-1, keep_dims=True)\n",
    "        shared_exps = tl.exp2(tl.floor(tl.log2(scales)) - 2) \n",
    "        x_had_scaled = x_had_grouped / shared_exps * (3/4) # 3/4 is constant. In CUDA, scale the GEMM output by 16/9\n",
    "    \n",
    "    # quantize\n",
    "    x_had_scaled_abs = tl.abs(x_had_scaled)\n",
    "    x_had_scaled_sign = tl.where(\n",
    "        x_had_scaled > 0,\n",
    "        1,\n",
    "        -1,\n",
    "    )\n",
    "    if stochastic_round:\n",
    "        x_fp4_high = tl.where(\n",
    "            x_had_scaled_abs > 4,\n",
    "            6,\n",
    "            tl.where(\n",
    "                x_had_scaled_abs > 3,\n",
    "                4,\n",
    "                tl.where(\n",
    "                    x_had_scaled_abs > 2,\n",
    "                    3,\n",
    "                    tl.where(\n",
    "                        x_had_scaled_abs > 1.5,\n",
    "                        2,\n",
    "                        tl.where(\n",
    "                            x_had_scaled_abs > 1.0,\n",
    "                            1.5,\n",
    "                            tl.where(\n",
    "                                x_had_scaled_abs > 0.5,\n",
    "                                1,\n",
    "                                0.5,\n",
    "                            )\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        x_fp4_low = tl.where(\n",
    "            x_had_scaled_abs > 4,\n",
    "            4,\n",
    "            tl.where(\n",
    "                x_had_scaled_abs > 3,\n",
    "                3,\n",
    "                tl.where(\n",
    "                    x_had_scaled_abs > 2,\n",
    "                    2,\n",
    "                    tl.where(\n",
    "                        x_had_scaled_abs > 1.5,\n",
    "                        1.5,\n",
    "                        tl.where(\n",
    "                            x_had_scaled_abs > 1.0,\n",
    "                            1.0,\n",
    "                            tl.where(\n",
    "                                x_had_scaled_abs > 0.5,\n",
    "                                0.5,\n",
    "                                0.0,\n",
    "                            )\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        prob_up = (x_had_scaled_abs - x_fp4_low) / (x_fp4_high - x_fp4_low)\n",
    "        sampled_prob = tl.rand(seed, offsets).reshape(BLOCK_SIZE // hadamard_dim, hadamard_dim)\n",
    "        x_fp4 = tl.where(\n",
    "            sampled_prob < prob_up,\n",
    "            x_fp4_high,\n",
    "            x_fp4_low,\n",
    "        ) * x_had_scaled_sign\n",
    "    else:    \n",
    "        x_fp4 = tl.where(\n",
    "            x_had_scaled_abs > 5,\n",
    "            6,\n",
    "            tl.where(\n",
    "                x_had_scaled_abs > 3.5,\n",
    "                4,\n",
    "                tl.where(\n",
    "                    x_had_scaled_abs > 2.5,\n",
    "                    3,\n",
    "                    tl.where(\n",
    "                        x_had_scaled_abs > 1.75,\n",
    "                        2,\n",
    "                        tl.where(\n",
    "                            x_had_scaled_abs > 1.25,\n",
    "                            1.5,\n",
    "                            tl.where(\n",
    "                                x_had_scaled_abs > 0.75,\n",
    "                                1,\n",
    "                                tl.where(\n",
    "                                    x_had_scaled_abs > 0.25,\n",
    "                                    0.5,\n",
    "                                    0,\n",
    "                                )\n",
    "                            )\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        ) * x_had_scaled_sign\n",
    "\n",
    "\n",
    "    # dequantize\n",
    "    if quest:\n",
    "        x_dequantized = x_fp4 * shared_exps\n",
    "        tl.store(\n",
    "            clip_mask_ptr + offsets,\n",
    "            tl.reshape(x_had_scaled_abs < 6, (BLOCK_SIZE,)),\n",
    "            mask=mask\n",
    "        )\n",
    "    else:\n",
    "        x_dequantized = x_fp4 * shared_exps * (4/3) # 3/4 is constant. In CUDA, scale the GEMM output by 16/9\n",
    "    \n",
    "    # Reshape back to flat form for storage\n",
    "    x_dequantized_flat = tl.reshape(x_dequantized, (BLOCK_SIZE,))\n",
    "    \n",
    "    # store\n",
    "    tl.store(output_ptr + offsets, x_dequantized_flat, mask=mask)\n",
    "\n",
    "\n",
    "def mxfp4_forward_kernel_wrapper(\n",
    "    x,\n",
    "    hadamard_matrix,\n",
    "    stochastic_round=False,\n",
    "    quest=True,\n",
    "):    \n",
    "    # Make sure inputs are contiguous\n",
    "    x = x.contiguous()\n",
    "    \n",
    "    # Create output tensor\n",
    "    output = torch.empty_like(x)\n",
    "    if quest:\n",
    "        clip_mask = torch.empty_like(x, dtype=torch.bool)\n",
    "    else:\n",
    "        clip_mask = None\n",
    "    \n",
    "    # Get total number of elements and calculate grid for launching the kernel\n",
    "    n_elements = x.numel()\n",
    "    grid = lambda meta: (triton.cdiv(n_elements, meta[\"BLOCK_SIZE\"]),)\n",
    "    \n",
    "    # Launch optimized kernel\n",
    "    mxfp4_forward_kernel[grid](\n",
    "        x_ptr=x,\n",
    "        hadamard_matrix_ptr=hadamard_matrix,\n",
    "        output_ptr=output,\n",
    "        clip_mask_ptr=clip_mask,\n",
    "        n_elements=n_elements,\n",
    "        hadamard_dim=hadamard_matrix.shape[-1],\n",
    "        group_size=32,\n",
    "        seed=42,\n",
    "        quest=quest,\n",
    "        stochastic_round=stochastic_round,\n",
    "    )\n",
    "    \n",
    "    return output, clip_mask\n",
    "\n",
    "\n",
    "class TritonGemm(torch.autograd.Function):\n",
    "    forward_hadamard_matrix = hadamard_transform(torch.eye(32, dtype=DTYPE, device=\"cuda\"), scale=32**(-1/2))\n",
    "    backward_hadamard_matrix = hadamard_transform(torch.eye(32, dtype=DTYPE, device=\"cuda\"), scale=32**(-1/2))\n",
    "    \n",
    "    def forward(ctx, input, weight, deterministic=False):\n",
    "        ctx.batch = input.shape[0]\n",
    "        ctx.seq = input.shape[1]\n",
    "        ctx.in_dim = weight.shape[1]\n",
    "        ctx.out_dim = weight.shape[0]\n",
    "        \n",
    "        ctx.deterministic = deterministic\n",
    "        \n",
    "        input_hf, input_mask_hf = mxfp4_forward_kernel_wrapper(\n",
    "            input,\n",
    "            TritonGemm.forward_hadamard_matrix,\n",
    "            stochastic_round=False,\n",
    "            quest=True,\n",
    "        )\n",
    "        weight_hf, weight_mask_hf = mxfp4_forward_kernel_wrapper(\n",
    "            weight,\n",
    "            TritonGemm.forward_hadamard_matrix,\n",
    "            stochastic_round=False,\n",
    "            quest=True,\n",
    "        )\n",
    "        \n",
    "        ctx.save_for_backward(input_hf, weight_hf, input_mask_hf, weight_mask_hf)\n",
    "        return F.linear(input_hf, weight_hf)\n",
    "    \n",
    "    @torch.compile()\n",
    "    def backward(ctx, grad_output):\n",
    "        input_hf, weight_hf, input_mask_hf, weight_mask_hf = ctx.saved_tensors\n",
    "        \n",
    "        if not ctx.deterministic:\n",
    "            TritonGemm.backward_hadamard_matrix = TritonGemm.backward_hadamard_matrix @ torch.diag(\n",
    "                torch.randint(\n",
    "                    0, 2, (32,),\n",
    "                    device=TritonGemm.backward_hadamard_matrix.device,\n",
    "                    dtype=TritonGemm.backward_hadamard_matrix.dtype\n",
    "                ) * 2 - 1\n",
    "            )\n",
    "        \n",
    "        grad_output_hb, _ = mxfp4_forward_kernel_wrapper(\n",
    "            grad_output,\n",
    "            TritonGemm.backward_hadamard_matrix,\n",
    "            stochastic_round=not ctx.deterministic,\n",
    "            quest=False,\n",
    "        )\n",
    "        hft_weightt_hb, _ = mxfp4_forward_kernel_wrapper(\n",
    "            weight_hf.T,\n",
    "            TritonGemm.backward_hadamard_matrix,\n",
    "            stochastic_round=not ctx.deterministic,\n",
    "            quest=False,\n",
    "        )\n",
    "        grad_input_hf = F.linear(grad_output_hb, hft_weightt_hb)\n",
    "        grad_input = (\n",
    "            (grad_input_hf.view(-1, 32) * input_mask_hf.view(-1, 32).to(grad_input_hf.dtype))\n",
    "            @ TritonGemm.forward_hadamard_matrix.T\n",
    "        ).view(ctx.batch, ctx.seq, ctx.in_dim)\n",
    "        \n",
    "        grad_outputt_hb, _ = mxfp4_forward_kernel_wrapper(\n",
    "            grad_output.view(-1, grad_output.size(-1)).T,\n",
    "            TritonGemm.backward_hadamard_matrix,\n",
    "            stochastic_round=not ctx.deterministic,\n",
    "            quest=False,\n",
    "        )\n",
    "        hft_inputt_hb, _ = mxfp4_forward_kernel_wrapper(\n",
    "            input_hf.view(-1, ctx.in_dim).T,\n",
    "            TritonGemm.backward_hadamard_matrix,\n",
    "            stochastic_round=not ctx.deterministic,\n",
    "            quest=False,\n",
    "        )\n",
    "        grad_weight_hf = F.linear(grad_outputt_hb, hft_inputt_hb)\n",
    "        grad_weight = (\n",
    "            (grad_weight_hf.view(-1, 32) * weight_mask_hf.view(-1, 32).to(grad_weight_hf.dtype))\n",
    "            @ TritonGemm.forward_hadamard_matrix.T\n",
    "        ).view(ctx.out_dim, ctx.in_dim)\n",
    "        return grad_input, grad_weight, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DETERMINISTIC_FOR_TESTS = True\n",
    "\n",
    "x = torch.randn(1, 32, 4096, device=\"cuda\", dtype=DTYPE, requires_grad=True)\n",
    "w = torch.randn(128, 4096, device=\"cuda\", dtype=DTYPE, requires_grad=True)\n",
    "\n",
    "\n",
    "y = F.linear(x, w)\n",
    "y_grad = torch.randn_like(y)\n",
    "y.backward(y_grad)\n",
    "\n",
    "grad = w.grad.clone()\n",
    "w.grad = None\n",
    "\n",
    "y_had = HadamardGemm.apply(x, w, DETERMINISTIC_FOR_TESTS)\n",
    "y_had.backward(y_grad)\n",
    "y_had_grad = w.grad.clone()\n",
    "w.grad = None\n",
    "\n",
    "y_fp4 = MXFP4Gemm.apply(x, w, DETERMINISTIC_FOR_TESTS)\n",
    "y_fp4.backward(y_grad)\n",
    "y_fp4_grad = w.grad.clone()\n",
    "w.grad = None\n",
    "\n",
    "y_triton = TritonGemm.apply(x, w, DETERMINISTIC_FOR_TESTS)\n",
    "y_triton.backward(y_grad)\n",
    "y_triton_grad = w.grad.clone()\n",
    "w.grad = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hadamard L2 error: 1.5e-05\n",
      "FP4 L2 error: 5.8e-02\n",
      "Triton L2 discrepancy: 1.4e-03\n",
      "Hadamard grad L2 error: 1.6e-05\n",
      "FP4 grad L2 error: 1.2e-01\n",
      "Triton grad L2 discrepancy: 4.4e-03\n"
     ]
    }
   ],
   "source": [
    "had_l2_error = (torch.linalg.norm(y - y_had) / torch.linalg.norm(y)).pow(2).detach().item()\n",
    "fp4_l2_error = (torch.linalg.norm(y - y_fp4) / torch.linalg.norm(y)).pow(2).detach().item()\n",
    "triton_l2_discrepancy = (torch.linalg.norm(y_fp4 - y_triton) / torch.linalg.norm(y)).pow(2).detach().item()\n",
    "\n",
    "print(f\"Hadamard L2 error: {had_l2_error:.1e}\")\n",
    "print(f\"FP4 L2 error: {fp4_l2_error:.1e}\")\n",
    "print(f\"Triton L2 discrepancy: {triton_l2_discrepancy:.1e}\")\n",
    "assert had_l2_error < 1e-4\n",
    "assert 2e-2 < fp4_l2_error < 6e-2\n",
    "assert triton_l2_discrepancy < fp4_l2_error / 10\n",
    "\n",
    "had_grad_l2_error = (torch.linalg.norm(grad - y_had_grad) / torch.linalg.norm(grad)).pow(2).detach().item()\n",
    "fp4_grad_l2_error = (torch.linalg.norm(grad - y_fp4_grad) / torch.linalg.norm(grad)).pow(2).detach().item()\n",
    "triton_grad_l2_discrepancy = (torch.linalg.norm(y_fp4_grad - y_triton_grad) / torch.linalg.norm(grad)).pow(2).detach().item()\n",
    "\n",
    "print(f\"Hadamard grad L2 error: {had_grad_l2_error:.1e}\")\n",
    "print(f\"FP4 grad L2 error: {fp4_grad_l2_error:.1e}\")\n",
    "print(f\"Triton grad L2 discrepancy: {triton_grad_l2_discrepancy:.1e}\")\n",
    "\n",
    "assert had_grad_l2_error < 1e-4\n",
    "assert 6e-2 < fp4_grad_l2_error < 15e-2\n",
    "assert triton_grad_l2_discrepancy < fp4_grad_l2_error / 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemm_fns = {\n",
    "    \"baseline\": F.linear,\n",
    "    \"+hadamard\": HadamardGemm.apply,\n",
    "    \"+mxfp4\": MXFP4Gemm.apply,\n",
    "    \"+triton\": TritonGemm.apply,\n",
    "}\n",
    "\n",
    "gemm_compiled_fns = {\n",
    "    k: torch.compile(v) for k, v in gemm_fns.items()\n",
    "}\n",
    "\n",
    "\n",
    "def benchmark_gpu(fn, input_size, weight_size, num_iterations=100, warmup=10):\n",
    "    \"\"\"Benchmark a function on GPU\"\"\"\n",
    "    input = torch.randn(*input_size, device=\"cuda\", dtype=DTYPE, requires_grad=True)\n",
    "    weight = torch.randn(*weight_size, device=\"cuda\", dtype=DTYPE, requires_grad=True)\n",
    "    \n",
    "    # Warmup\n",
    "    for _ in range(warmup):\n",
    "        result = fn(input, weight)\n",
    "    \n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    # Measure forward pass\n",
    "    start_time = time.time()\n",
    "    for _ in range(num_iterations):\n",
    "        result = fn(input, weight)\n",
    "        torch.cuda.synchronize()\n",
    "    forward_time = (time.time() - start_time) / num_iterations\n",
    "    \n",
    "    # Warmup\n",
    "    grad = torch.randn_like(result)\n",
    "    for _ in range(warmup):\n",
    "        result.backward(grad, retain_graph=True)\n",
    "    \n",
    "    # Measure backward pass\n",
    "    start_time = time.time()\n",
    "    for _ in range(num_iterations):\n",
    "        result = fn(input, weight)\n",
    "        result.backward(grad, retain_graph=True)\n",
    "        torch.cuda.synchronize()\n",
    "    backward_time = (time.time() - start_time) / num_iterations - forward_time\n",
    "    \n",
    "    return {\n",
    "        \"forward_ms\": forward_time * 1000,\n",
    "        \"backward_ms\": backward_time * 1000,\n",
    "        \"total_ms\": (forward_time + backward_time) * 1000\n",
    "    }\n",
    "\n",
    "\n",
    "def run_gpu_benchmarks(batch_size=64, seq_len=512, hidden_size=1024):\n",
    "    \"\"\"Run benchmarks for different GEMM implementations on GPU\"\"\"\n",
    "    input_size = (batch_size, seq_len, hidden_size)\n",
    "    weight_size = (hidden_size, hidden_size)\n",
    "    \n",
    "    results = {}\n",
    "    for name, fn in gemm_compiled_fns.items():\n",
    "        print(f\"Benchmarking {name}...\")\n",
    "        results[name] = benchmark_gpu(fn, input_size, weight_size)\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\nGPU Benchmark Results (ms):\")\n",
    "    print(f\"{'Method':<15} {'Forward':<10} {'Backward':<10} {'Total':<10}\")\n",
    "    print(\"-\" * 45)\n",
    "    for name, timings in results.items():\n",
    "        forward = f\"{timings['forward_ms']:.2f}\"\n",
    "        backward = f\"{timings['backward_ms']:.2f}\" if timings['backward_ms'] is not None else \"N/A\"\n",
    "        total = f\"{timings['total_ms']:.2f}\"\n",
    "        print(f\"{name:<15} {forward:<10} {backward:<10} {total:<10}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline...\n",
      "Benchmarking +hadamard...\n",
      "Benchmarking +mxfp4...\n",
      "Benchmarking +triton...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0506 16:08:09.656000 2094825 site-packages/torch/_dynamo/convert_frame.py:990] [5/8] torch._dynamo hit config.recompile_limit (8)\n",
      "W0506 16:08:09.656000 2094825 site-packages/torch/_dynamo/convert_frame.py:990] [5/8]    function: 'mxfp4_forward_kernel_wrapper' (/tmp/ipykernel_2094825/748202876.py:180)\n",
      "W0506 16:08:09.656000 2094825 site-packages/torch/_dynamo/convert_frame.py:990] [5/8]    last reason: 5/7: x._base.stride()[0] == x._base.size()[1]  # (unknown source x._base.stride()[0], please file a bug)\n",
      "W0506 16:08:09.656000 2094825 site-packages/torch/_dynamo/convert_frame.py:990] [5/8] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\n",
      "W0506 16:08:09.656000 2094825 site-packages/torch/_dynamo/convert_frame.py:990] [5/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GPU Benchmark Results (ms):\n",
      "Method          Forward    Backward   Total     \n",
      "---------------------------------------------\n",
      "baseline        5.13       12.55      17.68     \n",
      "+hadamard       5.83       18.73      24.56     \n",
      "+mxfp4          5.86       28.64      34.50     \n",
      "+triton         5.00       20.40      25.40     \n"
     ]
    }
   ],
   "source": [
    "_ = run_gpu_benchmarks(hidden_size=4096)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CUDA Kernels Needed\n",
    "\n",
    "---\n",
    "\n",
    "## Forward HT+Quant:\n",
    "\n",
    " * **Inputs**: \n",
    "    * `x`: torch.Tensor; shape=[M, K]; dtype=float32; already contiguous.\n",
    "    * `hadamard_matrix`: torch.Tensor; shape=[32, 32]; dtype=float32.\n",
    " * **Outputs**:\n",
    "    * `q`: torch.Tensor; shape=[M, K]; dtype=MXFP4 with scales along K;\n",
    "    * `mask`: torch.Tensor; shape=[M, K]; dtype=bool;\n",
    " * **Reference Impl**:\n",
    "    * Triton impl up to pseudoquant:\n",
    "        ```\n",
    "        q = mxfp4_forward_kernel_wrapper(\n",
    "            x,\n",
    "            hadamard_matrix,\n",
    "            stochastic_round=False,\n",
    "            quest=True,\n",
    "        )\n",
    "        ```\n",
    " * **Features**:\n",
    "    * Assume `M,K` divisible by 128.\n",
    "    * RTN projection.\n",
    "    * Scales based on STD.\n",
    "\n",
    "\n",
    "---\n",
    "    \n",
    "## Backward HT+Quant:\n",
    "\n",
    " * **Inputs**: \n",
    "    * `x`: torch.Tensor; shape=[M, K]; dtype=float32; already contiguous.\n",
    "    * `hadamard_matrix`: torch.Tensor; shape=[32, 32]; dtype=float32.\n",
    " * **Outputs**:\n",
    "    * `q`: torch.Tensor; shape=[M, K]; dtype=MXFP4 with scales along K;\n",
    " * **Reference Impl**:\n",
    "    * Triton impl up to pseudoquant:\n",
    "        ```\n",
    "        q = mxfp4_forward_kernel_wrapper(\n",
    "            x,\n",
    "            hadamard_matrix,\n",
    "            stochastic_round=True,\n",
    "            quest=False,\n",
    "        )\n",
    "        ```\n",
    " * **Features**:\n",
    "    * Assume `M,K` divisible by 128.\n",
    "    * Stochastic Rounding.\n",
    "    * Scales based on absmax.\n",
    "\n",
    "---\n",
    "\n",
    "## Backward Transpose+HT+Quant:\n",
    "\n",
    " * **Inputs**: \n",
    "    * `x`: torch.Tensor; shape=[M, K]; dtype=float32; already contiguous.\n",
    "    * `hadamard_matrix`: torch.Tensor; shape=[32, 32]; dtype=float32.\n",
    " * **Outputs**:\n",
    "    * `q`: torch.Tensor; shape=[K, M]; dtype=MXFP4 with scales along M;\n",
    " * **Reference Impl**:\n",
    "    * Triton impl up to pseudoquant:\n",
    "        ```\n",
    "        q = mxfp4_forward_kernel_wrapper(\n",
    "            x.T,\n",
    "            hadamard_matrix,\n",
    "            stochastic_round=True,\n",
    "            quest=False,\n",
    "        )\n",
    "        ```\n",
    " * **Features**:\n",
    "    * Assume `M,K` divisible by 128.\n",
    "    * Stochastic Rounding.\n",
    "    * Scales based on absmax.\n",
    "\n",
    "---\n",
    "    \n",
    "## Backward Dequant+Transpose+HT+Quant:\n",
    "\n",
    " * **Inputs**:\n",
    "    * `q`: torch.Tensor; shape=[M, K]; dtype=MXFP4 with scales along K;\n",
    "    * `hadamard_matrix`: torch.Tensor; shape=[32, 32]; dtype=float32.\n",
    " * **Outputs**:\n",
    "    * `qq`: torch.Tensor; shape=[K, M]; dtype=MXFP4 with scales along M;\n",
    " * **Reference Impl**:\n",
    "    * Triton impl up to pseudoquant:\n",
    "        ```\n",
    "        qq = mxfp4_forward_kernel_wrapper(\n",
    "            q.T,\n",
    "            hadamard_matrix,\n",
    "            stochastic_round=True,\n",
    "            quest=False,\n",
    "        )\n",
    "        ```\n",
    " * **Features**:\n",
    "    * Assume `M,K` divisible by 128.\n",
    "    * Stochastic Rounding.\n",
    "    * Scales based on absmax."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
