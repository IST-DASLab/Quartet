{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=0\n",
    "# %env TORCH_LOGS=recompiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch._dynamo.config.compiled_autograd = True\n",
    "torch._dynamo.config.recompile_limit = 2048\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import time\n",
    "from scipy.linalg import hadamard\n",
    "\n",
    "\n",
    "def get_hadamard_matrix(group_size: int, dtype: torch.dtype, device: torch.device):\n",
    "    return torch.tensor(\n",
    "        hadamard(group_size) * group_size**-0.5, dtype=dtype, device=device\n",
    "    )\n",
    "\n",
    "\n",
    "DTYPE = torch.bfloat16\n",
    "# DTYPE = torch.float32\n",
    "GRID = torch.tensor(\n",
    "    [-6.0, -4.0, -3.0, -2.0, -1.5, -1.0, -0.5, 0.0,\n",
    "    0.0,  0.5,  1.0,  1.5,  2.0,  3.0,  4.0, 6.0],\n",
    "    device=\"cuda\", dtype=DTYPE,\n",
    ")\n",
    "EMAX = 2\n",
    "SCALE = 3/4\n",
    "GAUSSIAL_SCALE = 2.92247856 / 6.0\n",
    "\n",
    "\n",
    "### FORWARD QUANTIZATION\n",
    "\n",
    "def rtn_fp4(x, grid):\n",
    "    inds = torch.bucketize(x, grid)\n",
    "\n",
    "    lo = torch.clamp(inds - 1, min=0, max=15)\n",
    "    hi = torch.clamp(inds,     min=0, max=15)\n",
    "\n",
    "    g_lo = grid[lo]\n",
    "    g_hi = grid[hi]\n",
    "\n",
    "    pick_hi = (g_hi - x) <= (x - g_lo)\n",
    "    return torch.where(pick_hi, g_hi, g_lo)\n",
    "\n",
    "\n",
    "@torch.compile\n",
    "def quantize_quest(x):\n",
    "    x_grouped = x.view(-1, 32)\n",
    "    shared_exps = torch.floor(torch.log2(\n",
    "        GAUSSIAL_SCALE * torch.std(x_grouped, dim=-1, correction=0, keepdim=True) + 1e-8\n",
    "    ))\n",
    "    scales = 2 ** shared_exps\n",
    "\n",
    "    scaled_x = x_grouped / scales\n",
    "\n",
    "    x_fp4 = rtn_fp4(scaled_x, GRID)\n",
    "\n",
    "    return (x_fp4 * scales).reshape_as(x), torch.abs(scaled_x) <= 6.0\n",
    "\n",
    "\n",
    "### BACKWARD QUANTIZATION\n",
    "\n",
    "def stochastic_round_fp4(x, grid):\n",
    "    inds = torch.bucketize(x, grid)\n",
    "\n",
    "    lo = torch.clamp(inds - 1, min=0, max=15)\n",
    "    hi = torch.clamp(inds,     min=0, max=15)\n",
    "\n",
    "    g_lo = grid[lo]\n",
    "    g_hi = grid[hi]\n",
    "\n",
    "    delta = g_hi - g_lo\n",
    "    p = torch.where(\n",
    "        delta > 0,\n",
    "        (x - g_lo) / delta,\n",
    "        torch.full_like(x, 0.5)\n",
    "    )\n",
    "\n",
    "    u = torch.rand_like(x)\n",
    "    pick_hi = u < p\n",
    "    return torch.where(pick_hi, g_hi, g_lo)\n",
    "\n",
    "\n",
    "@torch.compile\n",
    "def quantize_tseng(x):\n",
    "    x_grouped = x.view(-1, 32)\n",
    "    shared_exps = torch.floor(torch.log2(x_grouped.abs().max(dim=-1, keepdim=True)[0])) - EMAX\n",
    "    scales = 2 ** shared_exps / SCALE\n",
    "\n",
    "    scaled_x = x_grouped / scales\n",
    "\n",
    "    # x_fp4 = stochastic_round_fp4(scaled_x, GRID)\n",
    "    x_fp4 = rtn_fp4(scaled_x, GRID)\n",
    "\n",
    "    return (x_fp4 * scales).reshape_as(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "FORWARD_HADAMARD_MATRIX = get_hadamard_matrix(32, dtype=DTYPE, device=\"cuda\")\n",
    "BACKWARD_HADAMARD_MATRIX = get_hadamard_matrix(32, dtype=DTYPE, device=\"cuda\")\n",
    "\n",
    "class HadamardGemm(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, weight, deterministic=True):\n",
    "        ctx.batch = input.shape[0]\n",
    "        ctx.seq = input.shape[1]\n",
    "        ctx.in_dim = weight.shape[1]\n",
    "        ctx.out_dim = weight.shape[0]\n",
    "\n",
    "        ctx.deterministic = deterministic\n",
    "\n",
    "        input_hf = (\n",
    "            input.reshape(-1, 32) @ FORWARD_HADAMARD_MATRIX\n",
    "        ).view(ctx.batch, ctx.seq, ctx.in_dim)\n",
    "        weight_hf = (\n",
    "            weight.reshape(-1, 32) @ FORWARD_HADAMARD_MATRIX\n",
    "        ).view(ctx.out_dim, ctx.in_dim)\n",
    "\n",
    "        ctx.save_for_backward(input_hf, weight_hf)\n",
    "        return F.linear(input_hf, weight_hf)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        global BACKWARD_HADAMARD_MATRIX\n",
    "\n",
    "        input_hf, weight_hf = ctx.saved_tensors\n",
    "\n",
    "        if not ctx.deterministic:\n",
    "            BACKWARD_HADAMARD_MATRIX = BACKWARD_HADAMARD_MATRIX @ torch.diag(\n",
    "                torch.randint(\n",
    "                    0, 2, (32,),\n",
    "                    device=BACKWARD_HADAMARD_MATRIX.device,\n",
    "                    dtype=BACKWARD_HADAMARD_MATRIX.dtype\n",
    "                ) * 2 - 1\n",
    "            )\n",
    "\n",
    "        grad_output_hb = (\n",
    "            grad_output.reshape(-1, 32) @ BACKWARD_HADAMARD_MATRIX\n",
    "        ).view(ctx.batch, ctx.seq, ctx.out_dim)\n",
    "        hft_weightt_hb = (\n",
    "            weight_hf.T.reshape(-1, 32) @ BACKWARD_HADAMARD_MATRIX\n",
    "        ).view(ctx.in_dim, ctx.out_dim)\n",
    "        grad_input_hf = F.linear(grad_output_hb, hft_weightt_hb)\n",
    "        grad_input = (\n",
    "            grad_input_hf.reshape(-1, 32) @ FORWARD_HADAMARD_MATRIX.T\n",
    "        ).view(ctx.batch, ctx.seq, ctx.in_dim)\n",
    "\n",
    "        grad_outputt_hb = (\n",
    "            grad_output.view(-1, ctx.out_dim).T.reshape(-1, 32) @ BACKWARD_HADAMARD_MATRIX\n",
    "        ).view(ctx.out_dim, -1)\n",
    "        hft_inputt_hb = (\n",
    "            input_hf.view(-1, ctx.in_dim).T.reshape(-1, 32) @ BACKWARD_HADAMARD_MATRIX\n",
    "        ).view(ctx.in_dim, -1)\n",
    "        grad_weight_hf = F.linear(grad_outputt_hb, hft_inputt_hb)\n",
    "        grad_weight = (\n",
    "            grad_weight_hf.reshape(-1, 32) @ FORWARD_HADAMARD_MATRIX.T\n",
    "        ).view(ctx.out_dim, ctx.in_dim)\n",
    "        return grad_input, grad_weight, None\n",
    "\n",
    "\n",
    "class MXFP4Gemm(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, weight, deterministic=True):\n",
    "        ctx.batch = input.shape[0]\n",
    "        ctx.seq = input.shape[1]\n",
    "        ctx.in_dim = weight.shape[1]\n",
    "        ctx.out_dim = weight.shape[0]\n",
    "\n",
    "        ctx.deterministic = deterministic\n",
    "\n",
    "        input_hf, input_mask_hf = quantize_quest(\n",
    "            input.view(-1, 32) @ FORWARD_HADAMARD_MATRIX\n",
    "        )\n",
    "        input_hf, input_mask_hf = input_hf.view_as(input), input_mask_hf.view_as(input)\n",
    "        weight_hf, weight_mask_hf = quantize_quest(\n",
    "            weight.view(-1, 32) @ FORWARD_HADAMARD_MATRIX\n",
    "        )\n",
    "        weight_hf, weight_mask_hf = weight_hf.view_as(weight), weight_mask_hf.view_as(weight)\n",
    "\n",
    "        ctx.save_for_backward(input_hf, weight_hf, input_mask_hf, weight_mask_hf)\n",
    "        return F.linear(input_hf, weight_hf)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        global BACKWARD_HADAMARD_MATRIX\n",
    "        input_hf, weight_hf, input_mask_hf, weight_mask_hf = ctx.saved_tensors\n",
    "\n",
    "        if not ctx.deterministic:\n",
    "            BACKWARD_HADAMARD_MATRIX = BACKWARD_HADAMARD_MATRIX @ torch.diag(\n",
    "                torch.randint(\n",
    "                    0, 2, (32,),\n",
    "                    device=BACKWARD_HADAMARD_MATRIX.device,\n",
    "                    dtype=BACKWARD_HADAMARD_MATRIX.dtype\n",
    "                ) * 2 - 1\n",
    "            )\n",
    "\n",
    "        grad_output_hb = quantize_tseng(\n",
    "            grad_output.view(-1, 32) @ BACKWARD_HADAMARD_MATRIX.T\n",
    "        ).view(ctx.batch, ctx.seq, ctx.out_dim)\n",
    "        hft_weightt_hb = quantize_tseng(\n",
    "            weight_hf.T.reshape(-1, 32) @ BACKWARD_HADAMARD_MATRIX\n",
    "        ).view(ctx.in_dim, ctx.out_dim)\n",
    "        grad_input_hf = F.linear(grad_output_hb, hft_weightt_hb)\n",
    "        grad_input = (\n",
    "            (grad_input_hf.view(-1, 32) * input_mask_hf.view(-1, 32).to(grad_input_hf.dtype))\n",
    "            @ FORWARD_HADAMARD_MATRIX.T\n",
    "        ).view(ctx.batch, ctx.seq, ctx.in_dim)\n",
    "\n",
    "        grad_outputt_hb = quantize_tseng(\n",
    "            grad_output.view(-1, ctx.out_dim).T.reshape(-1, 32) @ BACKWARD_HADAMARD_MATRIX\n",
    "        ).view(ctx.out_dim, -1)\n",
    "        hft_inputt_hb = quantize_tseng(\n",
    "            input_hf.view(-1, ctx.in_dim).T.reshape(-1, 32) @ BACKWARD_HADAMARD_MATRIX\n",
    "        ).view(ctx.in_dim, -1)\n",
    "        grad_weight_hf = F.linear(grad_outputt_hb, hft_inputt_hb)\n",
    "        grad_weight = (\n",
    "            (grad_weight_hf.view(-1, 32) * weight_mask_hf.view(-1, 32).to(grad_weight_hf.dtype))\n",
    "            @ FORWARD_HADAMARD_MATRIX.T\n",
    "        ).view(ctx.out_dim, ctx.in_dim)\n",
    "        return grad_input, grad_weight, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "\n",
    "import torch\n",
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "\n",
    "@triton.autotune(\n",
    "    configs=[\n",
    "        triton.Config({\"BLOCK_SIZE\": 32 * 32}),\n",
    "        triton.Config({\"BLOCK_SIZE\": 64 * 32}),\n",
    "        triton.Config({\"BLOCK_SIZE\": 128 * 32}),\n",
    "        triton.Config({\"BLOCK_SIZE\": 256 * 32}),\n",
    "        triton.Config({\"BLOCK_SIZE\": 512 * 32}),\n",
    "    ],\n",
    "    key=[],\n",
    ")\n",
    "@triton.jit\n",
    "def mxfp4_forward_kernel(\n",
    "    x_ptr,\n",
    "    hadamard_matrix_ptr,\n",
    "    output_ptr,\n",
    "    clip_mask_ptr,\n",
    "    n_elements: tl.constexpr,\n",
    "    hadamard_dim: tl.constexpr,\n",
    "    group_size: tl.constexpr,\n",
    "    seed: int,\n",
    "    quest: tl.constexpr,\n",
    "    stochastic_round: tl.constexpr,\n",
    "    BLOCK_SIZE: tl.constexpr,\n",
    "):\n",
    "    offsets_hadamard = tl.arange(0, hadamard_dim * hadamard_dim)\n",
    "    hadamard_matrix = tl.load(hadamard_matrix_ptr + offsets_hadamard).reshape(hadamard_dim, hadamard_dim)\n",
    "\n",
    "    # load x\n",
    "    pid = tl.program_id(0)\n",
    "    start_idx = pid * BLOCK_SIZE\n",
    "    offsets = start_idx + tl.arange(0, BLOCK_SIZE)\n",
    "    mask = offsets < n_elements\n",
    "    x_flat = tl.load(x_ptr + offsets, mask=mask)\n",
    "\n",
    "    # hadamard transform\n",
    "    x = tl.reshape(x_flat, (BLOCK_SIZE // hadamard_dim, hadamard_dim))\n",
    "    x_had = tl.dot(x, hadamard_matrix)\n",
    "\n",
    "    # group\n",
    "    x_had_grouped = tl.reshape(x_had, (BLOCK_SIZE // group_size, group_size))\n",
    "\n",
    "    # scale\n",
    "    if quest:\n",
    "        mean_squared = tl.sum(x_had_grouped * x_had_grouped, axis=-1, keep_dims=True) / group_size\n",
    "        mean = tl.sum(x_had_grouped, axis=-1, keep_dims=True) / group_size\n",
    "        std = tl.sqrt(mean_squared - mean * mean)\n",
    "        scales = (2.92247856 / 6.0) * std + 1e-8\n",
    "        shared_exps = tl.exp2(tl.floor(tl.log2(scales)))\n",
    "        x_had_scaled = x_had_grouped / shared_exps\n",
    "    else:\n",
    "        scales = tl.max(tl.abs(x_had_grouped), axis=-1, keep_dims=True)\n",
    "        shared_exps = tl.exp2(tl.floor(tl.log2(scales)) - 2)\n",
    "        x_had_scaled = x_had_grouped / shared_exps * (3/4) # 3/4 is constant. In CUDA, scale the GEMM output by 16/9\n",
    "\n",
    "    # quantize\n",
    "    x_had_scaled_abs = tl.abs(x_had_scaled)\n",
    "    x_had_scaled_sign = tl.where(\n",
    "        x_had_scaled > 0,\n",
    "        1,\n",
    "        -1,\n",
    "    )\n",
    "    if stochastic_round:\n",
    "        x_fp4_high = tl.where(\n",
    "            x_had_scaled_abs > 4,\n",
    "            6,\n",
    "            tl.where(\n",
    "                x_had_scaled_abs > 3,\n",
    "                4,\n",
    "                tl.where(\n",
    "                    x_had_scaled_abs > 2,\n",
    "                    3,\n",
    "                    tl.where(\n",
    "                        x_had_scaled_abs > 1.5,\n",
    "                        2,\n",
    "                        tl.where(\n",
    "                            x_had_scaled_abs > 1.0,\n",
    "                            1.5,\n",
    "                            tl.where(\n",
    "                                x_had_scaled_abs > 0.5,\n",
    "                                1,\n",
    "                                0.5,\n",
    "                            )\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "        x_fp4_low = tl.where(\n",
    "            x_had_scaled_abs > 4,\n",
    "            4,\n",
    "            tl.where(\n",
    "                x_had_scaled_abs > 3,\n",
    "                3,\n",
    "                tl.where(\n",
    "                    x_had_scaled_abs > 2,\n",
    "                    2,\n",
    "                    tl.where(\n",
    "                        x_had_scaled_abs > 1.5,\n",
    "                        1.5,\n",
    "                        tl.where(\n",
    "                            x_had_scaled_abs > 1.0,\n",
    "                            1.0,\n",
    "                            tl.where(\n",
    "                                x_had_scaled_abs > 0.5,\n",
    "                                0.5,\n",
    "                                0.0,\n",
    "                            )\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "        prob_up = (x_had_scaled_abs - x_fp4_low) / (x_fp4_high - x_fp4_low)\n",
    "        sampled_prob = tl.rand(seed, offsets).reshape(BLOCK_SIZE // hadamard_dim, hadamard_dim)\n",
    "        x_fp4 = tl.where(\n",
    "            sampled_prob < prob_up,\n",
    "            x_fp4_high,\n",
    "            x_fp4_low,\n",
    "        ) * x_had_scaled_sign\n",
    "    else:\n",
    "        x_fp4 = tl.where(\n",
    "            x_had_scaled_abs > 5,\n",
    "            6,\n",
    "            tl.where(\n",
    "                x_had_scaled_abs > 3.5,\n",
    "                4,\n",
    "                tl.where(\n",
    "                    x_had_scaled_abs > 2.5,\n",
    "                    3,\n",
    "                    tl.where(\n",
    "                        x_had_scaled_abs > 1.75,\n",
    "                        2,\n",
    "                        tl.where(\n",
    "                            x_had_scaled_abs > 1.25,\n",
    "                            1.5,\n",
    "                            tl.where(\n",
    "                                x_had_scaled_abs > 0.75,\n",
    "                                1,\n",
    "                                tl.where(\n",
    "                                    x_had_scaled_abs > 0.25,\n",
    "                                    0.5,\n",
    "                                    0,\n",
    "                                )\n",
    "                            )\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        ) * x_had_scaled_sign\n",
    "\n",
    "\n",
    "    # dequantize\n",
    "    if quest:\n",
    "        x_dequantized = x_fp4 * shared_exps\n",
    "        tl.store(\n",
    "            clip_mask_ptr + offsets,\n",
    "            tl.reshape(x_had_scaled_abs < 6, (BLOCK_SIZE,)),\n",
    "            mask=mask\n",
    "        )\n",
    "    else:\n",
    "        x_dequantized = x_fp4 * shared_exps * (4/3) # 3/4 is constant. In CUDA, scale the GEMM output by 16/9\n",
    "\n",
    "    # Reshape back to flat form for storage\n",
    "    x_dequantized_flat = tl.reshape(x_dequantized, (BLOCK_SIZE,))\n",
    "\n",
    "    # store\n",
    "    tl.store(output_ptr + offsets, x_dequantized_flat, mask=mask)\n",
    "\n",
    "\n",
    "def mxfp4_forward_kernel_wrapper(\n",
    "    x,\n",
    "    hadamard_matrix,\n",
    "    stochastic_round=False,\n",
    "    quest=True,\n",
    "):\n",
    "    # Make sure inputs are contiguous\n",
    "    x = x.contiguous()\n",
    "\n",
    "    # Create output tensor\n",
    "    output = torch.empty_like(x)\n",
    "    if quest:\n",
    "        clip_mask = torch.empty_like(x, dtype=torch.bool)\n",
    "    else:\n",
    "        clip_mask = None\n",
    "\n",
    "    # Get total number of elements and calculate grid for launching the kernel\n",
    "    n_elements = x.numel()\n",
    "    grid = lambda meta: (triton.cdiv(n_elements, meta[\"BLOCK_SIZE\"]),)\n",
    "\n",
    "    # Launch optimized kernel\n",
    "    mxfp4_forward_kernel[grid](\n",
    "        x_ptr=x,\n",
    "        hadamard_matrix_ptr=hadamard_matrix,\n",
    "        output_ptr=output,\n",
    "        clip_mask_ptr=clip_mask,\n",
    "        n_elements=n_elements,\n",
    "        hadamard_dim=hadamard_matrix.shape[-1],\n",
    "        group_size=32,\n",
    "        seed=42,\n",
    "        quest=quest,\n",
    "        stochastic_round=stochastic_round,\n",
    "    )\n",
    "\n",
    "    return output, clip_mask\n",
    "\n",
    "\n",
    "class TritonGemm(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, weight, deterministic=True):\n",
    "        ctx.batch = input.shape[0]\n",
    "        ctx.seq = input.shape[1]\n",
    "        ctx.in_dim = weight.shape[1]\n",
    "        ctx.out_dim = weight.shape[0]\n",
    "\n",
    "        ctx.deterministic = deterministic\n",
    "\n",
    "        input_hf, input_mask_hf = mxfp4_forward_kernel_wrapper(\n",
    "            input,\n",
    "            FORWARD_HADAMARD_MATRIX,\n",
    "            stochastic_round=False,\n",
    "            quest=True,\n",
    "        )\n",
    "        weight_hf, weight_mask_hf = mxfp4_forward_kernel_wrapper(\n",
    "            weight,\n",
    "            FORWARD_HADAMARD_MATRIX,\n",
    "            stochastic_round=False,\n",
    "            quest=True,\n",
    "        )\n",
    "\n",
    "        ctx.save_for_backward(input_hf, weight_hf, input_mask_hf, weight_mask_hf)\n",
    "        return F.linear(input_hf, weight_hf)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        global BACKWARD_HADAMARD_MATRIX\n",
    "        input_hf, weight_hf, input_mask_hf, weight_mask_hf = ctx.saved_tensors\n",
    "\n",
    "        if not ctx.deterministic:\n",
    "            BACKWARD_HADAMARD_MATRIX = BACKWARD_HADAMARD_MATRIX @ torch.diag(\n",
    "                torch.randint(\n",
    "                    0, 2, (32,),\n",
    "                    device=BACKWARD_HADAMARD_MATRIX.device,\n",
    "                    dtype=BACKWARD_HADAMARD_MATRIX.dtype\n",
    "                ) * 2 - 1\n",
    "            )\n",
    "\n",
    "        grad_output_hb, _ = mxfp4_forward_kernel_wrapper(\n",
    "            grad_output,\n",
    "            BACKWARD_HADAMARD_MATRIX,\n",
    "            stochastic_round=not ctx.deterministic,\n",
    "            quest=False,\n",
    "        )\n",
    "        hft_weightt_hb, _ = mxfp4_forward_kernel_wrapper(\n",
    "            weight_hf.T,\n",
    "            BACKWARD_HADAMARD_MATRIX,\n",
    "            stochastic_round=not ctx.deterministic,\n",
    "            quest=False,\n",
    "        )\n",
    "        grad_input_hf = F.linear(grad_output_hb, hft_weightt_hb)\n",
    "        grad_input = (\n",
    "            (grad_input_hf.view(-1, 32) * input_mask_hf.view(-1, 32).to(grad_input_hf.dtype))\n",
    "            @ FORWARD_HADAMARD_MATRIX.T\n",
    "        ).view(ctx.batch, ctx.seq, ctx.in_dim)\n",
    "\n",
    "        grad_outputt_hb, _ = mxfp4_forward_kernel_wrapper(\n",
    "            grad_output.view(-1, grad_output.size(-1)).T,\n",
    "            BACKWARD_HADAMARD_MATRIX,\n",
    "            stochastic_round=not ctx.deterministic,\n",
    "            quest=False,\n",
    "        )\n",
    "        hft_inputt_hb, _ = mxfp4_forward_kernel_wrapper(\n",
    "            input_hf.view(-1, ctx.in_dim).T,\n",
    "            BACKWARD_HADAMARD_MATRIX,\n",
    "            stochastic_round=not ctx.deterministic,\n",
    "            quest=False,\n",
    "        )\n",
    "        grad_weight_hf = F.linear(grad_outputt_hb, hft_inputt_hb)\n",
    "        grad_weight = (\n",
    "            (grad_weight_hf.view(-1, 32) * weight_mask_hf.view(-1, 32).to(grad_weight_hf.dtype))\n",
    "            @ FORWARD_HADAMARD_MATRIX.T\n",
    "        ).view(ctx.out_dim, ctx.in_dim)\n",
    "        return grad_input, grad_weight, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apanfero/micromamba/envs/llmb/lib/python3.11/site-packages/torch/autograd/graph.py:841: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:270.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    }
   ],
   "source": [
    "DETERMINISTIC_FOR_TESTS = True\n",
    "\n",
    "x = torch.randn(1, 128, 4096, device=\"cuda\", dtype=DTYPE, requires_grad=True)\n",
    "w = torch.randn(128, 4096, device=\"cuda\", dtype=DTYPE, requires_grad=True)\n",
    "\n",
    "\n",
    "y = F.linear(x, w)\n",
    "y_grad = torch.randn_like(y)\n",
    "y.backward(y_grad)\n",
    "\n",
    "grad = w.grad.clone()\n",
    "w.grad = None\n",
    "\n",
    "y_had = HadamardGemm.apply(x, w, DETERMINISTIC_FOR_TESTS)\n",
    "y_had.backward(y_grad)\n",
    "y_had_grad = w.grad.clone()\n",
    "w.grad = None\n",
    "\n",
    "y_fp4 = MXFP4Gemm.apply(x, w, DETERMINISTIC_FOR_TESTS)\n",
    "y_fp4.backward(y_grad)\n",
    "y_fp4_grad = w.grad.clone()\n",
    "w.grad = None\n",
    "\n",
    "y_triton = TritonGemm.apply(x, w, DETERMINISTIC_FOR_TESTS)\n",
    "y_triton.backward(y_grad)\n",
    "y_triton_grad = w.grad.clone()\n",
    "w.grad = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hadamard L2 error: 1.5e-05\n",
      "FP4 L2 error: 5.6e-02\n",
      "Triton L2 discrepancy: 1.6e-03\n",
      "Hadamard grad L2 error: 1.6e-05\n",
      "FP4 grad L2 error: 1.2e-01\n",
      "Triton grad L2 discrepancy: 4.5e-03\n"
     ]
    }
   ],
   "source": [
    "had_l2_error = (torch.linalg.norm(y - y_had) / torch.linalg.norm(y)).pow(2).detach().item()\n",
    "fp4_l2_error = (torch.linalg.norm(y - y_fp4) / torch.linalg.norm(y)).pow(2).detach().item()\n",
    "triton_l2_discrepancy = (torch.linalg.norm(y_fp4 - y_triton) / torch.linalg.norm(y)).pow(2).detach().item()\n",
    "\n",
    "print(f\"Hadamard L2 error: {had_l2_error:.1e}\")\n",
    "print(f\"FP4 L2 error: {fp4_l2_error:.1e}\")\n",
    "print(f\"Triton L2 discrepancy: {triton_l2_discrepancy:.1e}\")\n",
    "assert had_l2_error < 1e-4\n",
    "assert 2e-2 < fp4_l2_error < 6e-2\n",
    "assert triton_l2_discrepancy < fp4_l2_error / 10\n",
    "\n",
    "had_grad_l2_error = (torch.linalg.norm(grad - y_had_grad) / torch.linalg.norm(grad)).pow(2).detach().item()\n",
    "fp4_grad_l2_error = (torch.linalg.norm(grad - y_fp4_grad) / torch.linalg.norm(grad)).pow(2).detach().item()\n",
    "triton_grad_l2_discrepancy = (torch.linalg.norm(y_fp4_grad - y_triton_grad) / torch.linalg.norm(grad)).pow(2).detach().item()\n",
    "\n",
    "print(f\"Hadamard grad L2 error: {had_grad_l2_error:.1e}\")\n",
    "print(f\"FP4 grad L2 error: {fp4_grad_l2_error:.1e}\")\n",
    "print(f\"Triton grad L2 discrepancy: {triton_grad_l2_discrepancy:.1e}\")\n",
    "\n",
    "assert had_grad_l2_error < 1e-4\n",
    "assert 6e-2 < fp4_grad_l2_error < 15e-2\n",
    "assert triton_grad_l2_discrepancy < fp4_grad_l2_error / 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemm_fns = {\n",
    "    \"baseline\": F.linear,\n",
    "    \"+hadamard\": HadamardGemm.apply,\n",
    "    \"+mxfp4\": MXFP4Gemm.apply,\n",
    "    \"+triton\": TritonGemm.apply,\n",
    "}\n",
    "\n",
    "gemm_compiled_fns = {\n",
    "    k: torch.compile(v) for k, v in gemm_fns.items()\n",
    "}\n",
    "\n",
    "\n",
    "def benchmark_gpu(fn, input_size, weight_size, num_iterations=100, warmup=10):\n",
    "    \"\"\"Benchmark a function on GPU\"\"\"\n",
    "    input = torch.randn(*input_size, device=\"cuda\", dtype=DTYPE, requires_grad=True)\n",
    "    weight = torch.randn(*weight_size, device=\"cuda\", dtype=DTYPE, requires_grad=True)\n",
    "\n",
    "    # # Warmup\n",
    "    # for _ in range(warmup):\n",
    "    #     result = fn(input, weight)\n",
    "\n",
    "    # torch.cuda.synchronize()\n",
    "\n",
    "    # # Measure forward pass\n",
    "    # start_time = time.time()\n",
    "    # for _ in range(num_iterations):\n",
    "    #     result = fn(input, weight)\n",
    "    #     torch.cuda.synchronize()\n",
    "    # forward_time = (time.time() - start_time) / num_iterations\n",
    "\n",
    "    # # Warmup\n",
    "    # grad = torch.randn_like(result)\n",
    "    # for _ in range(warmup):\n",
    "    #     result.backward(grad, retain_graph=True)\n",
    "\n",
    "    # # Measure backward pass\n",
    "    # start_time = time.time()\n",
    "    # for _ in range(num_iterations):\n",
    "    #     result = fn(input, weight)\n",
    "    #     result.backward(grad, retain_graph=True)\n",
    "    #     torch.cuda.synchronize()\n",
    "    # backward_time = (time.time() - start_time) / num_iterations - forward_time\n",
    "\n",
    "    # return {\n",
    "    #     \"forward_ms\": forward_time * 1000,\n",
    "    #     \"backward_ms\": backward_time * 1000,\n",
    "    #     \"total_ms\": (forward_time + backward_time) * 1000\n",
    "    # }\n",
    "    \n",
    "    result = fn(input, weight)\n",
    "    grad = torch.randn_like(result)\n",
    "    \n",
    "    forward_time = triton.testing.do_bench(\n",
    "        lambda: fn(input, weight), warmup=warmup, rep=num_iterations,\n",
    "    )\n",
    "    backward_time = triton.testing.do_bench(\n",
    "        lambda: result.backward(grad, retain_graph=True), warmup=warmup, rep=num_iterations,\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"forward_ms\": forward_time,\n",
    "        \"backward_ms\": backward_time,\n",
    "        \"total_ms\": (forward_time + backward_time)\n",
    "    }\n",
    "\n",
    "\n",
    "def run_gpu_benchmarks(batch_size=64, seq_len=512, hidden_size=1024):\n",
    "    \"\"\"Run benchmarks for different GEMM implementations on GPU\"\"\"\n",
    "    input_size = (batch_size, seq_len, hidden_size)\n",
    "    weight_size = (hidden_size, hidden_size)\n",
    "\n",
    "    results = {}\n",
    "    for name, fn in gemm_compiled_fns.items():\n",
    "        print(f\"Benchmarking {name}...\")\n",
    "        results[name] = benchmark_gpu(fn, input_size, weight_size)\n",
    "\n",
    "    # Print results\n",
    "    print(\"\\nGPU Benchmark Results (ms):\")\n",
    "    print(f\"{'Method':<15} {'Forward':<10} {'Backward':<10} {'Total':<10}\")\n",
    "    print(\"-\" * 45)\n",
    "    for name, timings in results.items():\n",
    "        forward = f\"{timings['forward_ms']:.2f}\"\n",
    "        backward = f\"{timings['backward_ms']:.2f}\" if timings['backward_ms'] is not None else \"N/A\"\n",
    "        total = f\"{timings['total_ms']:.2f}\"\n",
    "        print(f\"{name:<15} {forward:<10} {backward:<10} {total:<10}\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline...\n",
      "Benchmarking +hadamard...\n",
      "Benchmarking +mxfp4...\n",
      "Benchmarking +triton...\n",
      "\n",
      "GPU Benchmark Results (ms):\n",
      "Method          Forward    Backward   Total     \n",
      "---------------------------------------------\n",
      "baseline        5.06       11.43      16.50     \n",
      "+hadamard       6.33       13.65      19.98     \n",
      "+mxfp4          7.34       15.04      22.39     \n",
      "+triton         7.34       14.17      21.51     \n"
     ]
    }
   ],
   "source": [
    "_ = run_gpu_benchmarks(hidden_size=4096)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CUDA Kernels Needed\n",
    "\n",
    "---\n",
    "\n",
    "## Forward HT+Quant:\n",
    "\n",
    " * **Inputs**: \n",
    "    * `x`: torch.Tensor; shape=[M, K]; dtype=float32; already contiguous.\n",
    "    * `hadamard_matrix`: torch.Tensor; shape=[32, 32]; dtype=float32.\n",
    " * **Outputs**:\n",
    "    * `q`: torch.Tensor; shape=[M, K]; dtype=MXFP4 with scales along K;\n",
    "    * `mask`: torch.Tensor; shape=[M, K]; dtype=bool;\n",
    " * **Reference Impl**:\n",
    "    * Triton impl up to pseudoquant:\n",
    "        ```\n",
    "        q = mxfp4_forward_kernel_wrapper(\n",
    "            x,\n",
    "            hadamard_matrix,\n",
    "            stochastic_round=False,\n",
    "            quest=True,\n",
    "        )\n",
    "        ```\n",
    " * **Features**:\n",
    "    * Assume `M,K` divisible by 128.\n",
    "    * RTN projection.\n",
    "    * Scales based on STD.\n",
    "\n",
    "\n",
    "---\n",
    "    \n",
    "## Backward HT+Quant:\n",
    "\n",
    " * **Inputs**: \n",
    "    * `x`: torch.Tensor; shape=[M, K]; dtype=float32; already contiguous.\n",
    "    * `hadamard_matrix`: torch.Tensor; shape=[32, 32]; dtype=float32.\n",
    " * **Outputs**:\n",
    "    * `q`: torch.Tensor; shape=[M, K]; dtype=MXFP4 with scales along K;\n",
    " * **Reference Impl**:\n",
    "    * Triton impl up to pseudoquant:\n",
    "        ```\n",
    "        q = mxfp4_forward_kernel_wrapper(\n",
    "            x,\n",
    "            hadamard_matrix,\n",
    "            stochastic_round=True,\n",
    "            quest=False,\n",
    "        )\n",
    "        ```\n",
    " * **Features**:\n",
    "    * Assume `M,K` divisible by 128.\n",
    "    * Stochastic Rounding.\n",
    "    * Scales based on absmax.\n",
    "\n",
    "---\n",
    "\n",
    "## Backward Transpose+HT+Quant:\n",
    "\n",
    " * **Inputs**: \n",
    "    * `x`: torch.Tensor; shape=[M, K]; dtype=float32; already contiguous.\n",
    "    * `hadamard_matrix`: torch.Tensor; shape=[32, 32]; dtype=float32.\n",
    " * **Outputs**:\n",
    "    * `q`: torch.Tensor; shape=[K, M]; dtype=MXFP4 with scales along M;\n",
    " * **Reference Impl**:\n",
    "    * Triton impl up to pseudoquant:\n",
    "        ```\n",
    "        q = mxfp4_forward_kernel_wrapper(\n",
    "            x.T,\n",
    "            hadamard_matrix,\n",
    "            stochastic_round=True,\n",
    "            quest=False,\n",
    "        )\n",
    "        ```\n",
    " * **Features**:\n",
    "    * Assume `M,K` divisible by 128.\n",
    "    * Stochastic Rounding.\n",
    "    * Scales based on absmax.\n",
    "\n",
    "---\n",
    "    \n",
    "## Backward Dequant+Transpose+HT+Quant:\n",
    "\n",
    " * **Inputs**:\n",
    "    * `q`: torch.Tensor; shape=[M, K]; dtype=MXFP4 with scales along K;\n",
    "    * `hadamard_matrix`: torch.Tensor; shape=[32, 32]; dtype=float32.\n",
    " * **Outputs**:\n",
    "    * `qq`: torch.Tensor; shape=[K, M]; dtype=MXFP4 with scales along M;\n",
    " * **Reference Impl**:\n",
    "    * Triton impl up to pseudoquant:\n",
    "        ```\n",
    "        qq = mxfp4_forward_kernel_wrapper(\n",
    "            q.T,\n",
    "            hadamard_matrix,\n",
    "            stochastic_round=True,\n",
    "            quest=False,\n",
    "        )\n",
    "        ```\n",
    " * **Features**:\n",
    "    * Assume `M,K` divisible by 128.\n",
    "    * Stochastic Rounding.\n",
    "    * Scales based on absmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qutlass import matmul_mxf4_bf16_tn\n",
    "\n",
    "@torch.library.custom_op(\"quartet::matmul_mxf4_bf16_tn_op\", mutates_args=())\n",
    "def matmul_mxf4_bf16_tn_op(\n",
    "    x: torch.Tensor, w: torch.Tensor, xs: torch.Tensor, ws: torch.Tensor, alpha: torch.Tensor\n",
    ") -> torch.Tensor:\n",
    "    return matmul_mxf4_bf16_tn(\n",
    "        x.view(torch.uint8), w.view(torch.uint8), xs.view(torch.float8_e8m0fnu), ws.view(torch.float8_e8m0fnu), alpha\n",
    "    )\n",
    "\n",
    "@matmul_mxf4_bf16_tn_op.register_fake\n",
    "def _(x, w, xs, ws, alpha):\n",
    "    return x.new_empty(x.shape[0], w.shape[0], dtype=DTYPE)\n",
    "\n",
    "\n",
    "from qutlass import fusedQuantizeMx\n",
    "\n",
    "@torch.library.custom_op(\"quartet::fusedQuantizeMx_op\", mutates_args=())\n",
    "def fusedQuantizeMx_op(\n",
    "    x_flat: torch.Tensor, hadamard_matrix: torch.Tensor, return_mask: bool\n",
    ") -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    if return_mask:\n",
    "        return fusedQuantizeMx(x_flat, hadamard_matrix, return_mask=True)\n",
    "    else:\n",
    "        return fusedQuantizeMx(x_flat, hadamard_matrix, return_mask=False) + (None,)\n",
    "\n",
    "@fusedQuantizeMx_op.register_fake\n",
    "def _(x_flat, hadamard_matrix, return_mask):\n",
    "    rows, cols = x_flat.shape[0], x_flat.shape[1] // 32\n",
    "    padded_rows = ((rows + 128 - 1) // 128) * 128\n",
    "    padded_cols = ((cols + 4 - 1) // 4) * 4\n",
    "\n",
    "    xh_e2m1 = torch.empty(\n",
    "        x_flat.shape[0], x_flat.shape[1] // 2, dtype=torch.uint8, device=x_flat.device\n",
    "    )\n",
    "    xh_e8m0 = torch.empty(\n",
    "        padded_rows, padded_cols, dtype=torch.uint8, device=x_flat.device\n",
    "    )\n",
    "    clip_mask = torch.empty(*x_flat.shape[:-1], x_flat.size(-1) // 8,  dtype=torch.uint8, device=x_flat.device) if return_mask else None\n",
    "    return xh_e2m1, xh_e8m0, clip_mask\n",
    "\n",
    "\n",
    "from qutlass import backward_t_bf16\n",
    "\n",
    "@torch.library.custom_op(\"quartet::backward_t_bf16_op\", mutates_args=())\n",
    "def backward_t_bf16_op(\n",
    "    grad_output_flat: torch.Tensor, hadamard_matrix: torch.Tensor\n",
    ") -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    return backward_t_bf16(grad_output_flat, hadamard_matrix)\n",
    "\n",
    "@backward_t_bf16_op.register_fake\n",
    "def _(grad_output_flat, hadamard_matrix):\n",
    "    xh_e2m1 = torch.empty(grad_output_flat.shape[1], grad_output_flat.shape[0] // 2,  dtype=torch.uint8, device=grad_output_flat.device)\n",
    "    xh_e8m0 = torch.empty(grad_output_flat.shape[1], grad_output_flat.shape[0] // 32, dtype=torch.uint8, device=grad_output_flat.device)\n",
    "\n",
    "    return xh_e2m1, xh_e8m0\n",
    "\n",
    "\n",
    "from qutlass import backward_qt_bf16\n",
    "\n",
    "@torch.library.custom_op(\"quartet::backward_qt_bf16_op\", mutates_args=())\n",
    "def backward_qt_bf16_op(\n",
    "    x_e2m1: torch.Tensor,\n",
    "    x_e8m0: torch.Tensor,\n",
    "    h: torch.Tensor,\n",
    "    alpha: torch.Tensor,\n",
    ") -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    assert x_e2m1.dim() == 2\n",
    "    return backward_qt_bf16(x_e2m1, x_e8m0, h, alpha)\n",
    "\n",
    "@backward_qt_bf16_op.register_fake\n",
    "def _(x_e2m1, x_e8m0, h, alpha):\n",
    "    assert x_e2m1.dim() == 2\n",
    "    xh_e2m1 = torch.empty(x_e2m1.shape[1] * 2, x_e2m1.shape[0] // 2, dtype=torch.uint8, device=h.device)\n",
    "    xh_e8m0 = torch.empty(x_e8m0.shape[1] * 32, x_e8m0.shape[0] // 32, dtype=torch.uint8, device=h.device)\n",
    "    return xh_e2m1, xh_e8m0\n",
    "\n",
    "\n",
    "from qutlass import matmul_mxf8_bf16_tn\n",
    "\n",
    "@torch.library.custom_op(\"quartet::matmul_mxf8_bf16_tn_op\", mutates_args=())\n",
    "def matmul_mxf8_bf16_tn_op(\n",
    "    x: torch.Tensor, w: torch.Tensor, xs: torch.Tensor, ws: torch.Tensor, alpha: torch.Tensor\n",
    ") -> torch.Tensor:\n",
    "    return matmul_mxf8_bf16_tn(\n",
    "        x, w, xs.view(torch.float8_e8m0fnu), ws.view(torch.float8_e8m0fnu), alpha\n",
    "    )\n",
    "\n",
    "@matmul_mxf8_bf16_tn_op.register_fake\n",
    "def _(x, w, xs, ws, alpha):\n",
    "    return x.new_empty(x.shape[0], w.shape[0], dtype=DTYPE)\n",
    "\n",
    "\n",
    "from qutlass.utils import to_blocked\n",
    "\n",
    "def _unpack_mask(clip_mask: torch.Tensor) -> torch.Tensor:\n",
    "    clip_mask_unpacked_dq = torch.zeros(*clip_mask.shape[:-1], clip_mask.size(-1) * 8, dtype=torch.bool, device=clip_mask.device)\n",
    "    for i in range(8):\n",
    "        clip_mask_unpacked_dq[..., i::8] = (clip_mask >> i) & 1\n",
    "    return clip_mask_unpacked_dq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHA_FWD = torch.tensor(1., device=\"cuda\")\n",
    "ALPHA_BWD = torch.tensor(1./9., device=\"cuda\")\n",
    "\n",
    "class CudaGemm(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, weight, deterministic=True):\n",
    "        ctx.batch = input.shape[0]\n",
    "        ctx.seq = input.shape[1]\n",
    "        ctx.in_dim = weight.shape[1]\n",
    "        ctx.out_dim = weight.shape[0]\n",
    "\n",
    "        ctx.deterministic = deterministic\n",
    "\n",
    "        input_hf_e2m1, input_hf_e8m0, input_hf_mask = fusedQuantizeMx_op(\n",
    "            input.flatten(end_dim=-2),\n",
    "            FORWARD_HADAMARD_MATRIX,\n",
    "            return_mask=input.requires_grad,\n",
    "        )\n",
    "\n",
    "        weight_hf_e2m1, weight_hf_e8m0, weight_hf_mask = fusedQuantizeMx_op(\n",
    "            weight,\n",
    "            FORWARD_HADAMARD_MATRIX,\n",
    "            return_mask=input.requires_grad,\n",
    "        )\n",
    "\n",
    "        ctx.save_for_backward(input_hf_e2m1, input_hf_e8m0, input_hf_mask, weight_hf_e2m1, weight_hf_e8m0, weight_hf_mask)\n",
    "\n",
    "        input_hf_scale_block = to_blocked(input_hf_e8m0, False)\n",
    "        weight_hf_scale_block = to_blocked(weight_hf_e8m0, False)\n",
    "\n",
    "        out = matmul_mxf4_bf16_tn_op(\n",
    "            input_hf_e2m1,\n",
    "            weight_hf_e2m1,\n",
    "            input_hf_scale_block,\n",
    "            weight_hf_scale_block,\n",
    "            ALPHA_FWD,\n",
    "        )\n",
    "        return out.view(*input.shape[:-1], weight.size(-2))\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        global BACKWARD_HADAMARD_MATRIX\n",
    "        input_hf_e2m1, input_hf_e8m0, input_hf_mask, weight_hf_e2m1, weight_hf_e8m0, weight_hf_mask = ctx.saved_tensors\n",
    "\n",
    "        if not ctx.deterministic:\n",
    "            BACKWARD_HADAMARD_MATRIX = BACKWARD_HADAMARD_MATRIX * (\n",
    "                torch.randint(0, 2, (32,), device=BACKWARD_HADAMARD_MATRIX.device, dtype=BACKWARD_HADAMARD_MATRIX.dtype)\n",
    "                * 2. - 1.\n",
    "            )\n",
    "\n",
    "        grad_output_hb_e2m1, grad_output_hb_e8m0, _ = fusedQuantizeMx_op(\n",
    "            grad_output.flatten(end_dim=-2),\n",
    "            BACKWARD_HADAMARD_MATRIX,\n",
    "            False,\n",
    "        )\n",
    "\n",
    "        hft_weightt_hb_e2m1, hft_weightt_hb_e8m0 = backward_qt_bf16_op(weight_hf_e2m1, weight_hf_e8m0, BACKWARD_HADAMARD_MATRIX, ALPHA_FWD)\n",
    "        grad_output_hb_scale_block = to_blocked(grad_output_hb_e8m0, False)\n",
    "        hft_weightt_hb_scale_block = to_blocked(hft_weightt_hb_e8m0, False)\n",
    "        grad_input_hf = matmul_mxf4_bf16_tn_op(\n",
    "            grad_output_hb_e2m1,\n",
    "            hft_weightt_hb_e2m1,\n",
    "            grad_output_hb_scale_block,\n",
    "            hft_weightt_hb_scale_block,\n",
    "            ALPHA_BWD,\n",
    "        )\n",
    "\n",
    "        input_mask_hf = _unpack_mask(input_hf_mask)\n",
    "        grad_input = (\n",
    "            (grad_input_hf.view(-1, 32) * input_mask_hf.view(-1, 32).to(grad_input_hf.dtype))\n",
    "            @ FORWARD_HADAMARD_MATRIX.T\n",
    "        ).view(*grad_output.shape[:-1], weight_hf_e2m1.size(-1) * 2)\n",
    "\n",
    "        grad_outputt_hb_e2m1, grad_outputt_hb_e8m0 = backward_t_bf16_op(grad_output.flatten(end_dim=-2), BACKWARD_HADAMARD_MATRIX)\n",
    "        hft_inputt_hb_e2m1, hft_inputt_hb_e8m0 = backward_qt_bf16_op(input_hf_e2m1, input_hf_e8m0, BACKWARD_HADAMARD_MATRIX, ALPHA_FWD)\n",
    "        grad_outputt_hb_scale_block = to_blocked(grad_outputt_hb_e8m0, False)\n",
    "        hft_inputt_hb_scale_block = to_blocked(hft_inputt_hb_e8m0, False)\n",
    "        grad_weight_hf = matmul_mxf4_bf16_tn_op(\n",
    "            grad_outputt_hb_e2m1,\n",
    "            hft_inputt_hb_e2m1,\n",
    "            grad_outputt_hb_scale_block,\n",
    "            hft_inputt_hb_scale_block,\n",
    "            ALPHA_BWD,\n",
    "        )\n",
    "        \n",
    "        # torch._assert(grad_weight_hf.shape == (weight_hf_e2m1.size(0), weight_hf_e2m1.size(1) * 2), f\"{grad_outputt_hb_e2m1.shape=} {hft_inputt_hb_e2m1.shape=} {grad_weight_hf.shape=} {weight_hf_e2m1.shape=}\")\n",
    "\n",
    "        weight_mask_hf = _unpack_mask(weight_hf_mask)\n",
    "        grad_weight = (\n",
    "            (grad_weight_hf.view(-1, 32) * weight_mask_hf.view(-1, 32).to(grad_weight_hf.dtype))\n",
    "            @ FORWARD_HADAMARD_MATRIX.T\n",
    "        ).view(grad_output.size(-1), weight_hf_e2m1.size(-1) * 2)\n",
    "        return grad_input, grad_weight, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "DUMMY_E8M0 = torch.ones(2 ** 30, dtype=torch.float8_e8m0fnu, device=\"cuda\").view(torch.uint8)\n",
    "\n",
    "class Fp8Gemm(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def get_dummy_e8m0(x_e4m3: torch.Tensor) -> torch.Tensor:\n",
    "        x_e8m0 = DUMMY_E8M0[:x_e4m3.numel() // 32].view(*x_e4m3.shape[:-1], x_e4m3.size(-1) // 32)\n",
    "        return x_e8m0\n",
    "\n",
    "    @staticmethod\n",
    "    def mm_fp8(a_e4m3: torch.Tensor, b_e4m3: torch.Tensor) -> torch.Tensor:\n",
    "        c_bf16 = matmul_mxf8_bf16_tn_op(a_e4m3, b_e4m3, Fp8Gemm.get_dummy_e8m0(a_e4m3), Fp8Gemm.get_dummy_e8m0(b_e4m3), ALPHA_FWD)\n",
    "        return c_bf16\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, weight):\n",
    "        input_e4m3 = input.flatten(end_dim=-2).to(dtype=torch.float8_e4m3fn)\n",
    "        weight_e4m3 = weight.to(dtype=torch.float8_e4m3fn)\n",
    "        ctx.save_for_backward(input_e4m3, weight_e4m3)\n",
    "\n",
    "        return Fp8Gemm.mm_fp8(\n",
    "            input_e4m3,\n",
    "            weight_e4m3,\n",
    "        ).view(*input.shape[:-1], weight.size(-2))\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input_e4m3, weight_e4m3 = ctx.saved_tensors\n",
    "\n",
    "        grad_output_e4m3 = grad_output.flatten(end_dim=-2).to(dtype=torch.float8_e4m3fn)\n",
    "        \n",
    "        grad_input = Fp8Gemm.mm_fp8(\n",
    "            grad_output_e4m3,\n",
    "            weight_e4m3.T.contiguous(),\n",
    "        ).view(*grad_output.shape[:-1], weight_e4m3.size(-1))\n",
    "\n",
    "        grad_outputt_e4m3 = grad_output.flatten(end_dim=-2).to(dtype=torch.float8_e4m3fn)\n",
    "        grad_weight = Fp8Gemm.mm_fp8(\n",
    "            grad_outputt_e4m3.T.contiguous(),\n",
    "            input_e4m3.T.contiguous(),\n",
    "        ).view(grad_output.size(-1), weight_e4m3.size(-1))\n",
    "\n",
    "        return grad_input, grad_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_grad_enabled(True)\n",
    "\n",
    "y_cuda = CudaGemm.apply(x, w, DETERMINISTIC_FOR_TESTS)\n",
    "y_cuda.backward(y_grad)\n",
    "y_cuda_grad = w.grad.clone()\n",
    "w.grad = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_fp8 = Fp8Gemm.apply(x, w)\n",
    "y_fp8.backward(y_grad)\n",
    "y_fp8_grad = w.grad.clone()\n",
    "w.grad = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hadamard L2 error: 1.5e-05\n",
      "FP4 L2 error: 5.6e-02\n",
      "FP8 L2 error: 1.4e-03\n",
      "Triton L2 discrepancy: 1.6e-03\n",
      "Cuda L2 discrepancy: 1.6e-03\n",
      "Hadamard grad L2 error: 1.6e-05\n",
      "FP4 grad L2 error: 1.2e-01\n",
      "FP8 grad L2 error: 1.4e-03\n",
      "Triton grad L2 discrepancy: 4.5e-03\n",
      "Cuda grad L2 discrepancy: 4.5e-03\n"
     ]
    }
   ],
   "source": [
    "had_l2_error = (torch.linalg.norm(y - y_had) / torch.linalg.norm(y)).pow(2).detach().item()\n",
    "fp4_l2_error = (torch.linalg.norm(y - y_fp4) / torch.linalg.norm(y)).pow(2).detach().item()\n",
    "fp8_l2_error = (torch.linalg.norm(y - y_fp8) / torch.linalg.norm(y)).pow(2).detach().item()\n",
    "triton_l2_discrepancy = (torch.linalg.norm(y_fp4 - y_triton) / torch.linalg.norm(y)).pow(2).detach().item()\n",
    "cuda_l2_discrepancy = (torch.linalg.norm(y_fp4 - y_cuda) / torch.linalg.norm(y)).pow(2).detach().item()\n",
    "\n",
    "print(f\"Hadamard L2 error: {had_l2_error:.1e}\")\n",
    "print(f\"FP4 L2 error: {fp4_l2_error:.1e}\")\n",
    "print(f\"FP8 L2 error: {fp8_l2_error:.1e}\")\n",
    "print(f\"Triton L2 discrepancy: {triton_l2_discrepancy:.1e}\")\n",
    "print(f\"Cuda L2 discrepancy: {cuda_l2_discrepancy:.1e}\")\n",
    "assert had_l2_error < 1e-4\n",
    "assert fp8_l2_error < 2e-2 < fp4_l2_error < 6e-2\n",
    "assert triton_l2_discrepancy < fp4_l2_error / 10\n",
    "assert cuda_l2_discrepancy < fp4_l2_error / 10\n",
    "\n",
    "had_grad_l2_error = (torch.linalg.norm(grad - y_had_grad) / torch.linalg.norm(grad)).pow(2).detach().item()\n",
    "fp4_grad_l2_error = (torch.linalg.norm(grad - y_fp4_grad) / torch.linalg.norm(grad)).pow(2).detach().item()\n",
    "fp8_grad_l2_error = (torch.linalg.norm(grad - y_fp8_grad) / torch.linalg.norm(grad)).pow(2).detach().item()\n",
    "triton_grad_l2_discrepancy = (torch.linalg.norm(y_fp4_grad - y_triton_grad) / torch.linalg.norm(grad)).pow(2).detach().item()\n",
    "cuda_grad_l2_discrepancy = (torch.linalg.norm(y_fp4_grad - y_cuda_grad) / torch.linalg.norm(grad)).pow(2).detach().item()\n",
    "\n",
    "print(f\"Hadamard grad L2 error: {had_grad_l2_error:.1e}\")\n",
    "print(f\"FP4 grad L2 error: {fp4_grad_l2_error:.1e}\")\n",
    "print(f\"FP8 grad L2 error: {fp8_grad_l2_error:.1e}\")\n",
    "print(f\"Triton grad L2 discrepancy: {triton_grad_l2_discrepancy:.1e}\")\n",
    "print(f\"Cuda grad L2 discrepancy: {cuda_grad_l2_discrepancy:.1e}\")\n",
    "\n",
    "assert had_grad_l2_error < 1e-4\n",
    "assert fp8_grad_l2_error < 6e-2 < fp4_grad_l2_error < 15e-2\n",
    "assert triton_grad_l2_discrepancy < fp4_grad_l2_error / 10\n",
    "assert cuda_grad_l2_discrepancy < fp4_grad_l2_error / 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemm_fns = {\n",
    "    \"baseline\": F.linear,\n",
    "    # \"+hadamard\": HadamardGemm.apply,\n",
    "    # \"+mxfp4\": MXFP4Gemm.apply,\n",
    "    # \"+triton\": TritonGemm.apply,\n",
    "    \"+cuda\": CudaGemm.apply,\n",
    "    \"+fp8\": Fp8Gemm.apply,\n",
    "}\n",
    "\n",
    "torch._dynamo.config.compiled_autograd = True\n",
    "compile_kwargs = {\"fullgraph\": True}\n",
    "\n",
    "\n",
    "def benchmark_gpu(fn, input_size, weight_size, num_iterations=100, warmup=10):\n",
    "    \"\"\"Benchmark a function on GPU\"\"\"\n",
    "    input = torch.randn(*input_size, device=\"cuda\", dtype=DTYPE, requires_grad=True)\n",
    "    weight = torch.randn(*weight_size, device=\"cuda\", dtype=DTYPE, requires_grad=True)\n",
    "\n",
    "\n",
    "    # Forward\n",
    "    torch.set_grad_enabled(False)\n",
    "    \n",
    "    compiled_forward_fn = torch.compile(fn, **compile_kwargs)\n",
    "    \n",
    "    ms = triton.testing.do_bench(\n",
    "        lambda: compiled_forward_fn(input, weight), warmup=warmup, rep=num_iterations,\n",
    "    )\n",
    "    forward_time = ms\n",
    "\n",
    "    # Forward+Backward\n",
    "    grad = torch.randn_like(fn(input, weight))\n",
    "    torch.set_grad_enabled(True)    \n",
    "    \n",
    "    def compiled_forward_backward(input, weight, grad):\n",
    "        with torch._dynamo.compiled_autograd._enable(torch.compile(**compile_kwargs)):\n",
    "        # with torch._dynamo.utils.maybe_enable_compiled_autograd(True, dynamic=False, **compile_kwargs):\n",
    "            output = fn(input, weight)\n",
    "            output.backward(grad)\n",
    "    \n",
    "    ms = triton.testing.do_bench(\n",
    "        lambda: compiled_forward_backward(input, weight, grad), warmup=warmup, rep=num_iterations,\n",
    "    )\n",
    "    total_time = ms\n",
    "\n",
    "    return {\n",
    "        \"forward_ms\": forward_time,\n",
    "        \"total_ms\": total_time,\n",
    "    }\n",
    "\n",
    "\n",
    "def run_gpu_benchmarks(batch_size=64, seq_len=512, hidden_size=1024):\n",
    "    \"\"\"Run benchmarks for different GEMM implementations on GPU\"\"\"\n",
    "    input_size = (batch_size, seq_len, hidden_size)\n",
    "    weight_size = (hidden_size, hidden_size)\n",
    "\n",
    "    results = {}\n",
    "    for name, fn in gemm_fns.items():\n",
    "        print(f\"Benchmarking {name}...\")\n",
    "        results[name] = benchmark_gpu(fn, input_size, weight_size)\n",
    "\n",
    "    # Print results\n",
    "    print(\"\\nGPU Benchmark Results (ms):\")\n",
    "    print(f\"{'Method':<15} {'Forward':<10} {'Total':<10}\")\n",
    "    print(\"-\" * 45)\n",
    "    for name, timings in results.items():\n",
    "        forward = f\"{timings['forward_ms']:.2f}\"\n",
    "        total = f\"{timings['total_ms']:.2f}\"\n",
    "        print(f\"{name:<15} {forward:<10} {total:<10}\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline...\n",
      "Benchmarking +cuda...\n",
      "Benchmarking +fp8...\n",
      "\n",
      "GPU Benchmark Results (ms):\n",
      "Method          Forward    Total     \n",
      "---------------------------------------------\n",
      "baseline        0.38       1.25      \n",
      "+cuda           0.15       1.41      \n",
      "+fp8            0.21       0.89      \n"
     ]
    }
   ],
   "source": [
    "_ = run_gpu_benchmarks(hidden_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shapes = {\n",
    "#     # Q K V Down Up Gate Down\n",
    "#     \"100M\": [(1024 * 3, 1024), (1024, 1024), (2816 * 2, 1024), (1024, 2816)],\n",
    "#     \"800M\": [(2048 * 3, 2048), (2048, 2048), (5632 * 2, 2048), (2048, 5632)],\n",
    "#     \"3B\": [(3072 * 3, 3072), (3072, 3072), (8192 * 2, 3072), (3072, 8192)],\n",
    "#     \"7B\": [(4096 * 3, 4096), (4096, 4096), (11008 * 2, 4096), (4096, 11008)],\n",
    "#     \"22B\": [(6144 * 3, 6144), (6144, 6144), (16384 * 2, 6144), (6144, 16384)],\n",
    "#     \"52B\": [(8192 * 3, 8192), (8192, 8192), (22016 * 2, 8192), (8192, 22016)],\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "def from_num_head(n_head):\n",
    "    h = n_head * 128\n",
    "    inter = h * 8 / 3\n",
    "    inter = int((inter - 1) // 256) * 256 + 256\n",
    "    \n",
    "    shapes = [(h * 3, h), (h, h), (inter * 2, h), (h, inter)]\n",
    "    assert sum(map(lambda x: x[0] * x[1], shapes)) == 4 * h**2 + 3 * h * inter\n",
    "    \n",
    "    return (4 * h**2 + 3 * h * inter) * n_head, shapes\n",
    "\n",
    "\n",
    "shapes = {\n",
    "    n: shapes for (n, shapes) in map(from_num_head, chain(range(8, 13), range(16, 65, 4)))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS = {}\n",
    "\n",
    "def run_gpu_benchmarks_layer(batch_size=64, seq_len=512):\n",
    "    all_shapes = set()\n",
    "    for shape_list in shapes.values():\n",
    "        all_shapes.update(shape_list)\n",
    "    all_shapes = sorted(all_shapes)\n",
    "\n",
    "    for name in ['baseline', '+cuda', '+fp8']:\n",
    "        print(f\"Benchmarking {name}...\")\n",
    "        fn = gemm_fns[name]\n",
    "        if name not in RESULTS:\n",
    "            RESULTS[name] = {}\n",
    "        for weight_size in tqdm(all_shapes):\n",
    "            input_size = (batch_size, seq_len, weight_size[1])\n",
    "            if weight_size not in RESULTS[name]:\n",
    "                print(f\"Benchmarking {name} {weight_size}...\")\n",
    "                RESULTS[name][weight_size] = benchmark_gpu(fn, input_size, weight_size)\n",
    "            else:\n",
    "                print(f\"Skipping {name} {weight_size} because it already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/72 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (1024, 1024)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|         | 1/72 [00:00<00:16,  4.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (1024, 2816)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|         | 2/72 [00:02<01:39,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (1152, 1152)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|         | 3/72 [00:04<01:59,  1.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (1152, 3072)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|         | 4/72 [00:04<01:18,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (1280, 1280)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|         | 5/72 [00:05<00:54,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (1280, 3584)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|         | 6/72 [00:05<00:41,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (1408, 1408)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|         | 7/72 [00:05<00:32,  2.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (1408, 3840)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|         | 8/72 [00:05<00:26,  2.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (1536, 1536)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|        | 9/72 [00:06<00:22,  2.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (1536, 4096)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|        | 10/72 [00:06<00:20,  3.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (2048, 2048)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|        | 11/72 [00:06<00:18,  3.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (2048, 5632)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|        | 12/72 [00:06<00:18,  3.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (2560, 2560)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|        | 13/72 [00:07<00:17,  3.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (2560, 6912)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|        | 14/72 [00:07<00:17,  3.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (3072, 1024)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|        | 15/72 [00:07<00:16,  3.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (3072, 3072)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|       | 16/72 [00:08<00:16,  3.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (3072, 8192)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|       | 17/72 [00:08<00:17,  3.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (3456, 1152)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|       | 18/72 [00:08<00:16,  3.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (3584, 3584)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|       | 19/72 [00:08<00:15,  3.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (3584, 9728)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|       | 20/72 [00:09<00:18,  2.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (3840, 1280)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|       | 21/72 [00:09<00:16,  3.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (4096, 4096)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|       | 22/72 [00:10<00:16,  3.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (4096, 11008)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|      | 23/72 [00:10<00:19,  2.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (4224, 1408)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|      | 24/72 [00:10<00:17,  2.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (4608, 1536)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|      | 25/72 [00:11<00:15,  3.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (4608, 4608)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|      | 26/72 [00:11<00:15,  2.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (4608, 12288)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|      | 27/72 [00:12<00:19,  2.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (5120, 5120)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|      | 28/72 [00:12<00:18,  2.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (5120, 13824)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|      | 29/72 [00:13<00:22,  1.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (5632, 1024)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|     | 30/72 [00:13<00:18,  2.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (5632, 5632)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|     | 31/72 [00:14<00:18,  2.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (5632, 15104)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|     | 32/72 [00:14<00:22,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (6144, 1152)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|     | 33/72 [00:15<00:18,  2.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (6144, 2048)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|     | 34/72 [00:15<00:16,  2.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (6144, 6144)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|     | 35/72 [00:15<00:16,  2.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (6144, 16384)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|     | 36/72 [00:16<00:22,  1.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (6656, 6656)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|    | 37/72 [00:17<00:20,  1.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (6656, 17920)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|    | 38/72 [00:18<00:26,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (7168, 1280)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|    | 39/72 [00:18<00:20,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (7168, 7168)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|    | 40/72 [00:19<00:19,  1.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (7168, 19200)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|    | 41/72 [00:20<00:25,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (7680, 1408)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|    | 42/72 [00:21<00:20,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (7680, 2560)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|    | 43/72 [00:21<00:16,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (7680, 7680)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|    | 44/72 [00:22<00:16,  1.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (7680, 20480)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|   | 45/72 [00:23<00:23,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (8192, 1536)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|   | 46/72 [00:23<00:18,  1.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (8192, 8192)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|   | 47/72 [00:24<00:17,  1.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (8192, 22016)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|   | 48/72 [00:26<00:24,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (9216, 3072)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|   | 49/72 [00:26<00:19,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (10752, 3584)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|   | 50/72 [00:27<00:16,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (11264, 2048)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|   | 51/72 [00:27<00:13,  1.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (12288, 4096)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|  | 52/72 [00:28<00:12,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (13824, 2560)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|  | 53/72 [00:28<00:10,  1.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (13824, 4608)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|  | 54/72 [00:29<00:10,  1.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (15360, 5120)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|  | 55/72 [00:30<00:11,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (16384, 3072)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|  | 56/72 [00:30<00:10,  1.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (16896, 5632)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|  | 57/72 [00:31<00:11,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (18432, 6144)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|  | 58/72 [00:32<00:11,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (19456, 3584)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%| | 59/72 [00:33<00:10,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (19968, 6656)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%| | 60/72 [00:34<00:11,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (21504, 7168)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%| | 61/72 [00:36<00:12,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (22016, 4096)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%| | 62/72 [00:37<00:10,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (23040, 7680)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%| | 63/72 [00:39<00:11,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (24576, 4608)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%| | 64/72 [00:40<00:09,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (24576, 8192)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%| | 65/72 [00:42<00:09,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (27648, 5120)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|| 66/72 [00:43<00:08,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (30208, 5632)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|| 67/72 [00:45<00:07,  1.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (32768, 6144)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|| 68/72 [00:46<00:06,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (35840, 6656)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|| 69/72 [00:49<00:05,  1.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (38400, 7168)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|| 70/72 [00:51<00:04,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (40960, 7680)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|| 71/72 [00:54<00:02,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (44032, 8192)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 72/72 [00:58<00:00,  1.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/72 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (1024, 1024)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|         | 1/72 [00:00<00:15,  4.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (1024, 2816)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|         | 2/72 [00:07<05:18,  4.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (1152, 1152)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|         | 3/72 [00:23<10:58,  9.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (1152, 3072)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|         | 4/72 [00:23<06:39,  5.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (1280, 1280)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|         | 5/72 [00:23<04:16,  3.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (1280, 3584)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|         | 6/72 [00:23<02:52,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (1408, 1408)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|         | 7/72 [00:24<01:59,  1.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (1408, 3840)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|         | 8/72 [00:24<01:25,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (1536, 1536)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|        | 9/72 [00:24<01:02,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (1536, 4096)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|        | 10/72 [00:24<00:46,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (2048, 2048)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|        | 11/72 [00:25<00:36,  1.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (2048, 5632)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|        | 12/72 [00:25<00:29,  2.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (2560, 2560)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|        | 13/72 [00:25<00:24,  2.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (2560, 6912)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|        | 14/72 [00:25<00:22,  2.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (3072, 1024)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|        | 15/72 [00:26<00:19,  2.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (3072, 3072)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|       | 16/72 [00:26<00:17,  3.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (3072, 8192)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|       | 17/72 [00:26<00:16,  3.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (3456, 1152)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|       | 18/72 [00:27<00:15,  3.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (3584, 3584)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|       | 19/72 [00:27<00:14,  3.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (3584, 9728)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|       | 20/72 [00:27<00:15,  3.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (3840, 1280)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|       | 21/72 [00:27<00:14,  3.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (4096, 4096)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|       | 22/72 [00:28<00:13,  3.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (4096, 11008)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|      | 23/72 [00:28<00:14,  3.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (4224, 1408)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|      | 24/72 [00:28<00:13,  3.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (4608, 1536)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|      | 25/72 [00:28<00:12,  3.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (4608, 4608)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|      | 26/72 [00:29<00:12,  3.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (4608, 12288)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|      | 27/72 [00:29<00:13,  3.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (5120, 5120)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|      | 28/72 [00:29<00:13,  3.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (5120, 13824)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|      | 29/72 [00:30<00:14,  3.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (5632, 1024)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|     | 30/72 [00:30<00:12,  3.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (5632, 5632)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|     | 31/72 [00:30<00:12,  3.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (5632, 15104)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|     | 32/72 [00:31<00:13,  2.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (6144, 1152)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|     | 33/72 [00:31<00:12,  3.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (6144, 2048)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|     | 34/72 [00:31<00:11,  3.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (6144, 6144)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|     | 35/72 [00:32<00:11,  3.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (6144, 16384)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|     | 36/72 [00:32<00:12,  2.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (6656, 6656)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|    | 37/72 [00:32<00:12,  2.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (6656, 17920)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|    | 38/72 [00:33<00:13,  2.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (7168, 1280)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|    | 39/72 [00:33<00:11,  2.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (7168, 7168)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|    | 40/72 [00:34<00:11,  2.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (7168, 19200)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|    | 41/72 [00:34<00:13,  2.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (7680, 1408)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|    | 42/72 [00:34<00:11,  2.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (7680, 2560)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|    | 43/72 [00:35<00:09,  2.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (7680, 7680)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|    | 44/72 [00:35<00:09,  2.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (7680, 20480)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|   | 45/72 [00:36<00:11,  2.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (8192, 1536)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|   | 46/72 [00:36<00:09,  2.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (8192, 8192)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|   | 47/72 [00:36<00:09,  2.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (8192, 22016)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|   | 48/72 [00:37<00:11,  2.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (9216, 3072)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|   | 49/72 [00:37<00:09,  2.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (10752, 3584)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|   | 50/72 [00:38<00:08,  2.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (11264, 2048)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|   | 51/72 [00:38<00:07,  2.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (12288, 4096)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|  | 52/72 [00:38<00:07,  2.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (13824, 2560)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|  | 53/72 [00:39<00:06,  2.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (13824, 4608)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|  | 54/72 [00:39<00:06,  2.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (15360, 5120)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|  | 55/72 [00:39<00:06,  2.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (16384, 3072)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|  | 56/72 [00:40<00:05,  2.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (16896, 5632)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|  | 57/72 [00:40<00:05,  2.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (18432, 6144)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|  | 58/72 [00:41<00:05,  2.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (19456, 3584)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%| | 59/72 [00:41<00:05,  2.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (19968, 6656)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%| | 60/72 [00:41<00:05,  2.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (21504, 7168)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%| | 61/72 [00:42<00:05,  2.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (22016, 4096)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%| | 62/72 [00:42<00:04,  2.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (23040, 7680)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%| | 63/72 [00:43<00:04,  1.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (24576, 4608)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%| | 64/72 [00:44<00:03,  2.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (24576, 8192)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%| | 65/72 [00:44<00:03,  1.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (27648, 5120)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|| 66/72 [00:45<00:03,  1.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (30208, 5632)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|| 67/72 [00:45<00:02,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (32768, 6144)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|| 68/72 [00:46<00:02,  1.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (35840, 6656)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|| 69/72 [00:47<00:01,  1.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (38400, 7168)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|| 70/72 [00:48<00:01,  1.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (40960, 7680)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|| 71/72 [00:49<00:00,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (44032, 8192)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 72/72 [00:50<00:00,  1.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/72 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (1024, 1024)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|         | 1/72 [00:00<00:15,  4.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (1024, 2816)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|         | 2/72 [00:02<01:52,  1.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (1152, 1152)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|         | 3/72 [00:05<02:27,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (1152, 3072)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|         | 4/72 [00:05<01:34,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (1280, 1280)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|         | 5/72 [00:06<01:04,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (1280, 3584)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|         | 6/72 [00:06<00:47,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (1408, 1408)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|         | 7/72 [00:06<00:36,  1.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (1408, 3840)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|         | 8/72 [00:06<00:29,  2.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (1536, 1536)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|        | 9/72 [00:06<00:24,  2.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (1536, 4096)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|        | 10/72 [00:07<00:21,  2.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (2048, 2048)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|        | 11/72 [00:07<00:18,  3.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (2048, 5632)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|        | 12/72 [00:07<00:17,  3.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (2560, 2560)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|        | 13/72 [00:07<00:16,  3.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (2560, 6912)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|        | 14/72 [00:08<00:16,  3.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (3072, 1024)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|        | 15/72 [00:08<00:15,  3.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (3072, 3072)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|       | 16/72 [00:08<00:14,  3.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (3072, 8192)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|       | 17/72 [00:08<00:15,  3.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (3456, 1152)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|       | 18/72 [00:09<00:14,  3.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (3584, 3584)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|       | 19/72 [00:09<00:13,  3.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (3584, 9728)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|       | 20/72 [00:09<00:14,  3.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (3840, 1280)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|       | 21/72 [00:10<00:13,  3.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (4096, 4096)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|       | 22/72 [00:10<00:13,  3.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (4096, 11008)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|      | 23/72 [00:10<00:14,  3.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (4224, 1408)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|      | 24/72 [00:10<00:13,  3.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (4608, 1536)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|      | 25/72 [00:11<00:12,  3.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (4608, 4608)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|      | 26/72 [00:11<00:12,  3.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (4608, 12288)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|      | 27/72 [00:11<00:14,  3.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (5120, 5120)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|      | 28/72 [00:12<00:13,  3.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (5120, 13824)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|      | 29/72 [00:12<00:15,  2.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (5632, 1024)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|     | 30/72 [00:12<00:13,  3.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (5632, 5632)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|     | 31/72 [00:13<00:13,  3.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (5632, 15104)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|     | 32/72 [00:13<00:14,  2.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (6144, 1152)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|     | 33/72 [00:13<00:12,  3.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (6144, 2048)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|     | 34/72 [00:14<00:11,  3.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (6144, 6144)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|     | 35/72 [00:14<00:11,  3.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (6144, 16384)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|     | 36/72 [00:15<00:14,  2.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (6656, 6656)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|    | 37/72 [00:15<00:13,  2.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (6656, 17920)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|    | 38/72 [00:16<00:15,  2.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (7168, 1280)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|    | 39/72 [00:16<00:13,  2.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (7168, 7168)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|    | 40/72 [00:16<00:12,  2.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (7168, 19200)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|    | 41/72 [00:17<00:15,  1.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (7680, 1408)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|    | 42/72 [00:17<00:12,  2.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (7680, 2560)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|    | 43/72 [00:17<00:11,  2.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (7680, 7680)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|    | 44/72 [00:18<00:10,  2.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (7680, 20480)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|   | 45/72 [00:19<00:14,  1.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (8192, 1536)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|   | 46/72 [00:19<00:11,  2.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (8192, 8192)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|   | 47/72 [00:19<00:11,  2.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (8192, 22016)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|   | 48/72 [00:20<00:14,  1.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (9216, 3072)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|   | 49/72 [00:21<00:11,  1.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (10752, 3584)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|   | 50/72 [00:21<00:10,  2.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (11264, 2048)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|   | 51/72 [00:21<00:08,  2.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (12288, 4096)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|  | 52/72 [00:22<00:07,  2.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (13824, 2560)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|  | 53/72 [00:22<00:07,  2.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (13824, 4608)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|  | 54/72 [00:22<00:06,  2.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (15360, 5120)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|  | 55/72 [00:23<00:07,  2.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (16384, 3072)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|  | 56/72 [00:23<00:06,  2.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (16896, 5632)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|  | 57/72 [00:24<00:06,  2.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (18432, 6144)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|  | 58/72 [00:24<00:07,  2.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (19456, 3584)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%| | 59/72 [00:25<00:06,  2.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (19968, 6656)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%| | 60/72 [00:26<00:06,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (21504, 7168)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%| | 61/72 [00:27<00:07,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (22016, 4096)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%| | 62/72 [00:27<00:06,  1.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (23040, 7680)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%| | 63/72 [00:28<00:06,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (24576, 4608)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%| | 64/72 [00:29<00:05,  1.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (24576, 8192)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%| | 65/72 [00:30<00:05,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (27648, 5120)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|| 66/72 [00:31<00:04,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (30208, 5632)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|| 67/72 [00:32<00:04,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (32768, 6144)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|| 68/72 [00:33<00:03,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (35840, 6656)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|| 69/72 [00:34<00:03,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (38400, 7168)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|| 70/72 [00:36<00:02,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (40960, 7680)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|| 71/72 [00:37<00:01,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (44032, 8192)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 72/72 [00:39<00:00,  1.82it/s]\n"
     ]
    }
   ],
   "source": [
    "run_gpu_benchmarks_layer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quartet vs FP8 speedups:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{102760448: (1.6421884192100795, 0.5700544965507398),\n",
       " 143327232: (1.7507068972351185, 0.5428460629888379),\n",
       " 203161600: (1.816469204026392, 0.5630831684896017),\n",
       " 265650176: (1.8337929823925465, 0.5852455896793946),\n",
       " 339738624: (1.8373143452188634, 0.6084983531164263),\n",
       " 822083584: (1.94544993557363, 0.7102217258283788),\n",
       " 1585971200: (2.016410358863029, 0.7844006064602403),\n",
       " 2717908992: (2.04055388509346, 0.8540075062870569),\n",
       " 4367319040: (2.0577915015558075, 0.9298757910443761),\n",
       " 6476005376: (2.1262369945933037, 0.9957348479609069),\n",
       " 9172942848: (2.4712320149265006, 1.0801941994467603),\n",
       " 12687769600: (2.4354341004521824, 1.1340499333657252),\n",
       " 16811294720: (2.298580006518396, 1.1847066991846906),\n",
       " 21743271936: (2.2977885692458155, 1.2498281167452674),\n",
       " 27821867008: (2.284662048985173, 1.306921984970655),\n",
       " 34630270976: (2.2658029762562637, 1.3405308805024048),\n",
       " 42467328000: (2.1909100647863364, 1.3656890776400838),\n",
       " 51808043008: (2.127026201426419, 1.3737888056720073)}"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speedups = {}\n",
    "for model_size, tensor_shapes in shapes.items():\n",
    "    fp8_forward_latency = 0\n",
    "    fp8_backward_latency = 0\n",
    "    our_forward_latency = 0\n",
    "    our_backward_latency = 0\n",
    "    for key in tensor_shapes:\n",
    "        fp8_forward_latency += RESULTS['+fp8'][key][\"forward_ms\"]\n",
    "        fp8_backward_latency += RESULTS['+fp8'][key][\"total_ms\"] - RESULTS['+fp8'][key][\"forward_ms\"]\n",
    "        our_forward_latency += RESULTS['+cuda'][key][\"forward_ms\"]\n",
    "        our_backward_latency += RESULTS['+cuda'][key][\"total_ms\"] - RESULTS['+cuda'][key][\"forward_ms\"]\n",
    "    speedups[model_size] = (fp8_forward_latency / our_forward_latency, fp8_backward_latency/our_backward_latency)\n",
    "print(\"Quartet vs FP8 speedups:\\n\")\n",
    "speedups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quartet vs BF16 speedups:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{102760448: (3.5049470226024093, 0.8646094275384334),\n",
       " 143327232: (3.715967665970758, 0.9213451205718582),\n",
       " 203161600: (3.7432328443731238, 0.9898115334503184),\n",
       " 265650176: (3.9949393126868133, 1.1018071412556414),\n",
       " 339738624: (3.9681931586149175, 1.1402885192169),\n",
       " 822083584: (4.343412928598179, 1.3829555547147108),\n",
       " 1585971200: (4.53723356118794, 1.585533786598462),\n",
       " 2717908992: (4.6859598161053375, 1.777270307633622),\n",
       " 4367319040: (4.753695314032934, 1.9254506999337986),\n",
       " 6476005376: (4.777181752338976, 2.0752000752899664),\n",
       " 9172942848: (4.73225151546531, 2.195375242005379),\n",
       " 12687769600: (4.688234075118633, 2.3125219626443494),\n",
       " 16811294720: (4.32008093026321, 2.3977843198505044),\n",
       " 21743271936: (4.114755971002955, 2.480224805446807),\n",
       " 27821867008: (3.992000854253022, 2.5316271814060287),\n",
       " 34630270976: (3.9050340451224814, 2.6144872925084353),\n",
       " 42467328000: (3.756596215509311, 2.6503525698540353),\n",
       " 51808043008: (3.6367347234294076, 2.7054283751005626)}"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speedups = {}\n",
    "for model_size, tensor_shapes in shapes.items():\n",
    "    fp8_forward_latency = 0\n",
    "    fp8_backward_latency = 0\n",
    "    our_forward_latency = 0\n",
    "    our_backward_latency = 0\n",
    "    for key in tensor_shapes:\n",
    "        fp8_forward_latency += RESULTS['baseline'][key][\"forward_ms\"]\n",
    "        fp8_backward_latency += RESULTS['baseline'][key][\"total_ms\"] - RESULTS['baseline'][key][\"forward_ms\"]\n",
    "        our_forward_latency += RESULTS['+cuda'][key][\"forward_ms\"]\n",
    "        our_backward_latency += RESULTS['+cuda'][key][\"total_ms\"] - RESULTS['+cuda'][key][\"forward_ms\"]\n",
    "    speedups[model_size] = (fp8_forward_latency / our_forward_latency, fp8_backward_latency/our_backward_latency)\n",
    "print(\"Quartet vs BF16 speedups:\\n\")\n",
    "speedups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paste them into plots.ipynb!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
