{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=0\n",
    "# %env TORCH_LOGS=recompiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch._dynamo.config.compiled_autograd = True\n",
    "torch._dynamo.config.recompile_limit = 2048\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import time\n",
    "from scipy.linalg import hadamard\n",
    "\n",
    "\n",
    "def get_hadamard_matrix(group_size: int, dtype: torch.dtype, device: torch.device):\n",
    "    return torch.tensor(\n",
    "        hadamard(group_size) * group_size**-0.5, dtype=dtype, device=device\n",
    "    )\n",
    "\n",
    "\n",
    "DTYPE = torch.bfloat16\n",
    "# DTYPE = torch.float32\n",
    "GRID = torch.tensor(\n",
    "    [-6.0, -4.0, -3.0, -2.0, -1.5, -1.0, -0.5, 0.0,\n",
    "    0.0,  0.5,  1.0,  1.5,  2.0,  3.0,  4.0, 6.0],\n",
    "    device=\"cuda\", dtype=DTYPE,\n",
    ")\n",
    "EMAX = 2\n",
    "SCALE = 3/4\n",
    "GAUSSIAL_SCALE = 2.92247856 / 6.0\n",
    "\n",
    "\n",
    "### FORWARD QUANTIZATION\n",
    "\n",
    "def rtn_fp4(x, grid):\n",
    "    inds = torch.bucketize(x, grid)\n",
    "\n",
    "    lo = torch.clamp(inds - 1, min=0, max=15)\n",
    "    hi = torch.clamp(inds,     min=0, max=15)\n",
    "\n",
    "    g_lo = grid[lo]\n",
    "    g_hi = grid[hi]\n",
    "\n",
    "    pick_hi = (g_hi - x) <= (x - g_lo)\n",
    "    return torch.where(pick_hi, g_hi, g_lo)\n",
    "\n",
    "\n",
    "@torch.compile\n",
    "def quantize_quest(x):\n",
    "    x_grouped = x.view(-1, 32)\n",
    "    shared_exps = torch.floor(torch.log2(\n",
    "        GAUSSIAL_SCALE * torch.std(x_grouped, dim=-1, correction=0, keepdim=True) + 1e-8\n",
    "    ))\n",
    "    scales = 2 ** shared_exps\n",
    "\n",
    "    scaled_x = x_grouped / scales\n",
    "\n",
    "    x_fp4 = rtn_fp4(scaled_x, GRID)\n",
    "\n",
    "    return (x_fp4 * scales).reshape_as(x), torch.abs(scaled_x) <= 6.0\n",
    "\n",
    "\n",
    "### BACKWARD QUANTIZATION\n",
    "\n",
    "def stochastic_round_fp4(x, grid):\n",
    "    inds = torch.bucketize(x, grid)\n",
    "\n",
    "    lo = torch.clamp(inds - 1, min=0, max=15)\n",
    "    hi = torch.clamp(inds,     min=0, max=15)\n",
    "\n",
    "    g_lo = grid[lo]\n",
    "    g_hi = grid[hi]\n",
    "\n",
    "    delta = g_hi - g_lo\n",
    "    p = torch.where(\n",
    "        delta > 0,\n",
    "        (x - g_lo) / delta,\n",
    "        torch.full_like(x, 0.5)\n",
    "    )\n",
    "\n",
    "    u = torch.rand_like(x)\n",
    "    pick_hi = u < p\n",
    "    return torch.where(pick_hi, g_hi, g_lo)\n",
    "\n",
    "\n",
    "@torch.compile\n",
    "def quantize_tseng(x):\n",
    "    x_grouped = x.view(-1, 32)\n",
    "    shared_exps = torch.floor(torch.log2(x_grouped.abs().max(dim=-1, keepdim=True)[0])) - EMAX\n",
    "    scales = 2 ** shared_exps / SCALE\n",
    "\n",
    "    scaled_x = x_grouped / scales\n",
    "\n",
    "    # x_fp4 = stochastic_round_fp4(scaled_x, GRID)\n",
    "    x_fp4 = rtn_fp4(scaled_x, GRID)\n",
    "\n",
    "    return (x_fp4 * scales).reshape_as(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "FORWARD_HADAMARD_MATRIX = get_hadamard_matrix(32, dtype=DTYPE, device=\"cuda\")\n",
    "BACKWARD_HADAMARD_MATRIX = get_hadamard_matrix(32, dtype=DTYPE, device=\"cuda\")\n",
    "\n",
    "class HadamardGemm(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, weight, deterministic=True):\n",
    "        ctx.batch = input.shape[0]\n",
    "        ctx.seq = input.shape[1]\n",
    "        ctx.in_dim = weight.shape[1]\n",
    "        ctx.out_dim = weight.shape[0]\n",
    "\n",
    "        ctx.deterministic = deterministic\n",
    "\n",
    "        input_hf = (\n",
    "            input.reshape(-1, 32) @ FORWARD_HADAMARD_MATRIX\n",
    "        ).view(ctx.batch, ctx.seq, ctx.in_dim)\n",
    "        weight_hf = (\n",
    "            weight.reshape(-1, 32) @ FORWARD_HADAMARD_MATRIX\n",
    "        ).view(ctx.out_dim, ctx.in_dim)\n",
    "\n",
    "        ctx.save_for_backward(input_hf, weight_hf)\n",
    "        return F.linear(input_hf, weight_hf)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        global BACKWARD_HADAMARD_MATRIX\n",
    "\n",
    "        input_hf, weight_hf = ctx.saved_tensors\n",
    "\n",
    "        if not ctx.deterministic:\n",
    "            BACKWARD_HADAMARD_MATRIX = BACKWARD_HADAMARD_MATRIX @ torch.diag(\n",
    "                torch.randint(\n",
    "                    0, 2, (32,),\n",
    "                    device=BACKWARD_HADAMARD_MATRIX.device,\n",
    "                    dtype=BACKWARD_HADAMARD_MATRIX.dtype\n",
    "                ) * 2 - 1\n",
    "            )\n",
    "\n",
    "        grad_output_hb = (\n",
    "            grad_output.reshape(-1, 32) @ BACKWARD_HADAMARD_MATRIX\n",
    "        ).view(ctx.batch, ctx.seq, ctx.out_dim)\n",
    "        hft_weightt_hb = (\n",
    "            weight_hf.T.reshape(-1, 32) @ BACKWARD_HADAMARD_MATRIX\n",
    "        ).view(ctx.in_dim, ctx.out_dim)\n",
    "        grad_input_hf = F.linear(grad_output_hb, hft_weightt_hb)\n",
    "        grad_input = (\n",
    "            grad_input_hf.reshape(-1, 32) @ FORWARD_HADAMARD_MATRIX.T\n",
    "        ).view(ctx.batch, ctx.seq, ctx.in_dim)\n",
    "\n",
    "        grad_outputt_hb = (\n",
    "            grad_output.view(-1, ctx.out_dim).T.reshape(-1, 32) @ BACKWARD_HADAMARD_MATRIX\n",
    "        ).view(ctx.out_dim, -1)\n",
    "        hft_inputt_hb = (\n",
    "            input_hf.view(-1, ctx.in_dim).T.reshape(-1, 32) @ BACKWARD_HADAMARD_MATRIX\n",
    "        ).view(ctx.in_dim, -1)\n",
    "        grad_weight_hf = F.linear(grad_outputt_hb, hft_inputt_hb)\n",
    "        grad_weight = (\n",
    "            grad_weight_hf.reshape(-1, 32) @ FORWARD_HADAMARD_MATRIX.T\n",
    "        ).view(ctx.out_dim, ctx.in_dim)\n",
    "        return grad_input, grad_weight, None\n",
    "\n",
    "\n",
    "class MXFP4Gemm(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, weight, deterministic=True):\n",
    "        ctx.batch = input.shape[0]\n",
    "        ctx.seq = input.shape[1]\n",
    "        ctx.in_dim = weight.shape[1]\n",
    "        ctx.out_dim = weight.shape[0]\n",
    "\n",
    "        ctx.deterministic = deterministic\n",
    "\n",
    "        input_hf, input_mask_hf = quantize_quest(\n",
    "            input.view(-1, 32) @ FORWARD_HADAMARD_MATRIX\n",
    "        )\n",
    "        input_hf, input_mask_hf = input_hf.view_as(input), input_mask_hf.view_as(input)\n",
    "        weight_hf, weight_mask_hf = quantize_quest(\n",
    "            weight.view(-1, 32) @ FORWARD_HADAMARD_MATRIX\n",
    "        )\n",
    "        weight_hf, weight_mask_hf = weight_hf.view_as(weight), weight_mask_hf.view_as(weight)\n",
    "\n",
    "        ctx.save_for_backward(input_hf, weight_hf, input_mask_hf, weight_mask_hf)\n",
    "        return F.linear(input_hf, weight_hf)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        global BACKWARD_HADAMARD_MATRIX\n",
    "        input_hf, weight_hf, input_mask_hf, weight_mask_hf = ctx.saved_tensors\n",
    "\n",
    "        if not ctx.deterministic:\n",
    "            BACKWARD_HADAMARD_MATRIX = BACKWARD_HADAMARD_MATRIX @ torch.diag(\n",
    "                torch.randint(\n",
    "                    0, 2, (32,),\n",
    "                    device=BACKWARD_HADAMARD_MATRIX.device,\n",
    "                    dtype=BACKWARD_HADAMARD_MATRIX.dtype\n",
    "                ) * 2 - 1\n",
    "            )\n",
    "\n",
    "        grad_output_hb = quantize_tseng(\n",
    "            grad_output.view(-1, 32) @ BACKWARD_HADAMARD_MATRIX.T\n",
    "        ).view(ctx.batch, ctx.seq, ctx.out_dim)\n",
    "        hft_weightt_hb = quantize_tseng(\n",
    "            weight_hf.T.reshape(-1, 32) @ BACKWARD_HADAMARD_MATRIX\n",
    "        ).view(ctx.in_dim, ctx.out_dim)\n",
    "        grad_input_hf = F.linear(grad_output_hb, hft_weightt_hb)\n",
    "        grad_input = (\n",
    "            (grad_input_hf.view(-1, 32) * input_mask_hf.view(-1, 32).to(grad_input_hf.dtype))\n",
    "            @ FORWARD_HADAMARD_MATRIX.T\n",
    "        ).view(ctx.batch, ctx.seq, ctx.in_dim)\n",
    "\n",
    "        grad_outputt_hb = quantize_tseng(\n",
    "            grad_output.view(-1, ctx.out_dim).T.reshape(-1, 32) @ BACKWARD_HADAMARD_MATRIX\n",
    "        ).view(ctx.out_dim, -1)\n",
    "        hft_inputt_hb = quantize_tseng(\n",
    "            input_hf.view(-1, ctx.in_dim).T.reshape(-1, 32) @ BACKWARD_HADAMARD_MATRIX\n",
    "        ).view(ctx.in_dim, -1)\n",
    "        grad_weight_hf = F.linear(grad_outputt_hb, hft_inputt_hb)\n",
    "        grad_weight = (\n",
    "            (grad_weight_hf.view(-1, 32) * weight_mask_hf.view(-1, 32).to(grad_weight_hf.dtype))\n",
    "            @ FORWARD_HADAMARD_MATRIX.T\n",
    "        ).view(ctx.out_dim, ctx.in_dim)\n",
    "        return grad_input, grad_weight, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "\n",
    "import torch\n",
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "\n",
    "@triton.autotune(\n",
    "    configs=[\n",
    "        triton.Config({\"BLOCK_SIZE\": 32 * 32}),\n",
    "        triton.Config({\"BLOCK_SIZE\": 64 * 32}),\n",
    "        triton.Config({\"BLOCK_SIZE\": 128 * 32}),\n",
    "        triton.Config({\"BLOCK_SIZE\": 256 * 32}),\n",
    "        triton.Config({\"BLOCK_SIZE\": 512 * 32}),\n",
    "    ],\n",
    "    key=[],\n",
    ")\n",
    "@triton.jit\n",
    "def mxfp4_forward_kernel(\n",
    "    x_ptr,\n",
    "    hadamard_matrix_ptr,\n",
    "    output_ptr,\n",
    "    clip_mask_ptr,\n",
    "    n_elements: tl.constexpr,\n",
    "    hadamard_dim: tl.constexpr,\n",
    "    group_size: tl.constexpr,\n",
    "    seed: int,\n",
    "    quest: tl.constexpr,\n",
    "    stochastic_round: tl.constexpr,\n",
    "    BLOCK_SIZE: tl.constexpr,\n",
    "):\n",
    "    offsets_hadamard = tl.arange(0, hadamard_dim * hadamard_dim)\n",
    "    hadamard_matrix = tl.load(hadamard_matrix_ptr + offsets_hadamard).reshape(hadamard_dim, hadamard_dim)\n",
    "\n",
    "    # load x\n",
    "    pid = tl.program_id(0)\n",
    "    start_idx = pid * BLOCK_SIZE\n",
    "    offsets = start_idx + tl.arange(0, BLOCK_SIZE)\n",
    "    mask = offsets < n_elements\n",
    "    x_flat = tl.load(x_ptr + offsets, mask=mask)\n",
    "\n",
    "    # hadamard transform\n",
    "    x = tl.reshape(x_flat, (BLOCK_SIZE // hadamard_dim, hadamard_dim))\n",
    "    x_had = tl.dot(x, hadamard_matrix)\n",
    "\n",
    "    # group\n",
    "    x_had_grouped = tl.reshape(x_had, (BLOCK_SIZE // group_size, group_size))\n",
    "\n",
    "    # scale\n",
    "    if quest:\n",
    "        mean_squared = tl.sum(x_had_grouped * x_had_grouped, axis=-1, keep_dims=True) / group_size\n",
    "        mean = tl.sum(x_had_grouped, axis=-1, keep_dims=True) / group_size\n",
    "        std = tl.sqrt(mean_squared - mean * mean)\n",
    "        scales = (2.92247856 / 6.0) * std + 1e-8\n",
    "        shared_exps = tl.exp2(tl.floor(tl.log2(scales)))\n",
    "        x_had_scaled = x_had_grouped / shared_exps\n",
    "    else:\n",
    "        scales = tl.max(tl.abs(x_had_grouped), axis=-1, keep_dims=True)\n",
    "        shared_exps = tl.exp2(tl.floor(tl.log2(scales)) - 2)\n",
    "        x_had_scaled = x_had_grouped / shared_exps * (3/4) # 3/4 is constant. In CUDA, scale the GEMM output by 16/9\n",
    "\n",
    "    # quantize\n",
    "    x_had_scaled_abs = tl.abs(x_had_scaled)\n",
    "    x_had_scaled_sign = tl.where(\n",
    "        x_had_scaled > 0,\n",
    "        1,\n",
    "        -1,\n",
    "    )\n",
    "    if stochastic_round:\n",
    "        x_fp4_high = tl.where(\n",
    "            x_had_scaled_abs > 4,\n",
    "            6,\n",
    "            tl.where(\n",
    "                x_had_scaled_abs > 3,\n",
    "                4,\n",
    "                tl.where(\n",
    "                    x_had_scaled_abs > 2,\n",
    "                    3,\n",
    "                    tl.where(\n",
    "                        x_had_scaled_abs > 1.5,\n",
    "                        2,\n",
    "                        tl.where(\n",
    "                            x_had_scaled_abs > 1.0,\n",
    "                            1.5,\n",
    "                            tl.where(\n",
    "                                x_had_scaled_abs > 0.5,\n",
    "                                1,\n",
    "                                0.5,\n",
    "                            )\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "        x_fp4_low = tl.where(\n",
    "            x_had_scaled_abs > 4,\n",
    "            4,\n",
    "            tl.where(\n",
    "                x_had_scaled_abs > 3,\n",
    "                3,\n",
    "                tl.where(\n",
    "                    x_had_scaled_abs > 2,\n",
    "                    2,\n",
    "                    tl.where(\n",
    "                        x_had_scaled_abs > 1.5,\n",
    "                        1.5,\n",
    "                        tl.where(\n",
    "                            x_had_scaled_abs > 1.0,\n",
    "                            1.0,\n",
    "                            tl.where(\n",
    "                                x_had_scaled_abs > 0.5,\n",
    "                                0.5,\n",
    "                                0.0,\n",
    "                            )\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "        prob_up = (x_had_scaled_abs - x_fp4_low) / (x_fp4_high - x_fp4_low)\n",
    "        sampled_prob = tl.rand(seed, offsets).reshape(BLOCK_SIZE // hadamard_dim, hadamard_dim)\n",
    "        x_fp4 = tl.where(\n",
    "            sampled_prob < prob_up,\n",
    "            x_fp4_high,\n",
    "            x_fp4_low,\n",
    "        ) * x_had_scaled_sign\n",
    "    else:\n",
    "        x_fp4 = tl.where(\n",
    "            x_had_scaled_abs > 5,\n",
    "            6,\n",
    "            tl.where(\n",
    "                x_had_scaled_abs > 3.5,\n",
    "                4,\n",
    "                tl.where(\n",
    "                    x_had_scaled_abs > 2.5,\n",
    "                    3,\n",
    "                    tl.where(\n",
    "                        x_had_scaled_abs > 1.75,\n",
    "                        2,\n",
    "                        tl.where(\n",
    "                            x_had_scaled_abs > 1.25,\n",
    "                            1.5,\n",
    "                            tl.where(\n",
    "                                x_had_scaled_abs > 0.75,\n",
    "                                1,\n",
    "                                tl.where(\n",
    "                                    x_had_scaled_abs > 0.25,\n",
    "                                    0.5,\n",
    "                                    0,\n",
    "                                )\n",
    "                            )\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        ) * x_had_scaled_sign\n",
    "\n",
    "\n",
    "    # dequantize\n",
    "    if quest:\n",
    "        x_dequantized = x_fp4 * shared_exps\n",
    "        tl.store(\n",
    "            clip_mask_ptr + offsets,\n",
    "            tl.reshape(x_had_scaled_abs < 6, (BLOCK_SIZE,)),\n",
    "            mask=mask\n",
    "        )\n",
    "    else:\n",
    "        x_dequantized = x_fp4 * shared_exps * (4/3) # 3/4 is constant. In CUDA, scale the GEMM output by 16/9\n",
    "\n",
    "    # Reshape back to flat form for storage\n",
    "    x_dequantized_flat = tl.reshape(x_dequantized, (BLOCK_SIZE,))\n",
    "\n",
    "    # store\n",
    "    tl.store(output_ptr + offsets, x_dequantized_flat, mask=mask)\n",
    "\n",
    "\n",
    "def mxfp4_forward_kernel_wrapper(\n",
    "    x,\n",
    "    hadamard_matrix,\n",
    "    stochastic_round=False,\n",
    "    quest=True,\n",
    "):\n",
    "    # Make sure inputs are contiguous\n",
    "    x = x.contiguous()\n",
    "\n",
    "    # Create output tensor\n",
    "    output = torch.empty_like(x)\n",
    "    if quest:\n",
    "        clip_mask = torch.empty_like(x, dtype=torch.bool)\n",
    "    else:\n",
    "        clip_mask = None\n",
    "\n",
    "    # Get total number of elements and calculate grid for launching the kernel\n",
    "    n_elements = x.numel()\n",
    "    grid = lambda meta: (triton.cdiv(n_elements, meta[\"BLOCK_SIZE\"]),)\n",
    "\n",
    "    # Launch optimized kernel\n",
    "    mxfp4_forward_kernel[grid](\n",
    "        x_ptr=x,\n",
    "        hadamard_matrix_ptr=hadamard_matrix,\n",
    "        output_ptr=output,\n",
    "        clip_mask_ptr=clip_mask,\n",
    "        n_elements=n_elements,\n",
    "        hadamard_dim=hadamard_matrix.shape[-1],\n",
    "        group_size=32,\n",
    "        seed=42,\n",
    "        quest=quest,\n",
    "        stochastic_round=stochastic_round,\n",
    "    )\n",
    "\n",
    "    return output, clip_mask\n",
    "\n",
    "\n",
    "class TritonGemm(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, weight, deterministic=True):\n",
    "        ctx.batch = input.shape[0]\n",
    "        ctx.seq = input.shape[1]\n",
    "        ctx.in_dim = weight.shape[1]\n",
    "        ctx.out_dim = weight.shape[0]\n",
    "\n",
    "        ctx.deterministic = deterministic\n",
    "\n",
    "        input_hf, input_mask_hf = mxfp4_forward_kernel_wrapper(\n",
    "            input,\n",
    "            FORWARD_HADAMARD_MATRIX,\n",
    "            stochastic_round=False,\n",
    "            quest=True,\n",
    "        )\n",
    "        weight_hf, weight_mask_hf = mxfp4_forward_kernel_wrapper(\n",
    "            weight,\n",
    "            FORWARD_HADAMARD_MATRIX,\n",
    "            stochastic_round=False,\n",
    "            quest=True,\n",
    "        )\n",
    "\n",
    "        ctx.save_for_backward(input_hf, weight_hf, input_mask_hf, weight_mask_hf)\n",
    "        return F.linear(input_hf, weight_hf)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        global BACKWARD_HADAMARD_MATRIX\n",
    "        input_hf, weight_hf, input_mask_hf, weight_mask_hf = ctx.saved_tensors\n",
    "\n",
    "        if not ctx.deterministic:\n",
    "            BACKWARD_HADAMARD_MATRIX = BACKWARD_HADAMARD_MATRIX @ torch.diag(\n",
    "                torch.randint(\n",
    "                    0, 2, (32,),\n",
    "                    device=BACKWARD_HADAMARD_MATRIX.device,\n",
    "                    dtype=BACKWARD_HADAMARD_MATRIX.dtype\n",
    "                ) * 2 - 1\n",
    "            )\n",
    "\n",
    "        grad_output_hb, _ = mxfp4_forward_kernel_wrapper(\n",
    "            grad_output,\n",
    "            BACKWARD_HADAMARD_MATRIX,\n",
    "            stochastic_round=not ctx.deterministic,\n",
    "            quest=False,\n",
    "        )\n",
    "        hft_weightt_hb, _ = mxfp4_forward_kernel_wrapper(\n",
    "            weight_hf.T,\n",
    "            BACKWARD_HADAMARD_MATRIX,\n",
    "            stochastic_round=not ctx.deterministic,\n",
    "            quest=False,\n",
    "        )\n",
    "        grad_input_hf = F.linear(grad_output_hb, hft_weightt_hb)\n",
    "        grad_input = (\n",
    "            (grad_input_hf.view(-1, 32) * input_mask_hf.view(-1, 32).to(grad_input_hf.dtype))\n",
    "            @ FORWARD_HADAMARD_MATRIX.T\n",
    "        ).view(ctx.batch, ctx.seq, ctx.in_dim)\n",
    "\n",
    "        grad_outputt_hb, _ = mxfp4_forward_kernel_wrapper(\n",
    "            grad_output.view(-1, grad_output.size(-1)).T,\n",
    "            BACKWARD_HADAMARD_MATRIX,\n",
    "            stochastic_round=not ctx.deterministic,\n",
    "            quest=False,\n",
    "        )\n",
    "        hft_inputt_hb, _ = mxfp4_forward_kernel_wrapper(\n",
    "            input_hf.view(-1, ctx.in_dim).T,\n",
    "            BACKWARD_HADAMARD_MATRIX,\n",
    "            stochastic_round=not ctx.deterministic,\n",
    "            quest=False,\n",
    "        )\n",
    "        grad_weight_hf = F.linear(grad_outputt_hb, hft_inputt_hb)\n",
    "        grad_weight = (\n",
    "            (grad_weight_hf.view(-1, 32) * weight_mask_hf.view(-1, 32).to(grad_weight_hf.dtype))\n",
    "            @ FORWARD_HADAMARD_MATRIX.T\n",
    "        ).view(ctx.out_dim, ctx.in_dim)\n",
    "        return grad_input, grad_weight, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apanfero/micromamba/envs/llmb/lib/python3.11/site-packages/torch/autograd/graph.py:841: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:270.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    }
   ],
   "source": [
    "DETERMINISTIC_FOR_TESTS = True\n",
    "\n",
    "x = torch.randn(1, 128, 4096, device=\"cuda\", dtype=DTYPE, requires_grad=True)\n",
    "w = torch.randn(128, 4096, device=\"cuda\", dtype=DTYPE, requires_grad=True)\n",
    "\n",
    "\n",
    "y = F.linear(x, w)\n",
    "y_grad = torch.randn_like(y)\n",
    "y.backward(y_grad)\n",
    "\n",
    "grad = w.grad.clone()\n",
    "w.grad = None\n",
    "\n",
    "y_had = HadamardGemm.apply(x, w, DETERMINISTIC_FOR_TESTS)\n",
    "y_had.backward(y_grad)\n",
    "y_had_grad = w.grad.clone()\n",
    "w.grad = None\n",
    "\n",
    "y_fp4 = MXFP4Gemm.apply(x, w, DETERMINISTIC_FOR_TESTS)\n",
    "y_fp4.backward(y_grad)\n",
    "y_fp4_grad = w.grad.clone()\n",
    "w.grad = None\n",
    "\n",
    "y_triton = TritonGemm.apply(x, w, DETERMINISTIC_FOR_TESTS)\n",
    "y_triton.backward(y_grad)\n",
    "y_triton_grad = w.grad.clone()\n",
    "w.grad = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hadamard L2 error: 1.5e-05\n",
      "FP4 L2 error: 5.7e-02\n",
      "Triton L2 discrepancy: 1.5e-03\n",
      "Hadamard grad L2 error: 1.6e-05\n",
      "FP4 grad L2 error: 1.2e-01\n",
      "Triton grad L2 discrepancy: 4.5e-03\n"
     ]
    }
   ],
   "source": [
    "had_l2_error = (torch.linalg.norm(y - y_had) / torch.linalg.norm(y)).pow(2).detach().item()\n",
    "fp4_l2_error = (torch.linalg.norm(y - y_fp4) / torch.linalg.norm(y)).pow(2).detach().item()\n",
    "triton_l2_discrepancy = (torch.linalg.norm(y_fp4 - y_triton) / torch.linalg.norm(y)).pow(2).detach().item()\n",
    "\n",
    "print(f\"Hadamard L2 error: {had_l2_error:.1e}\")\n",
    "print(f\"FP4 L2 error: {fp4_l2_error:.1e}\")\n",
    "print(f\"Triton L2 discrepancy: {triton_l2_discrepancy:.1e}\")\n",
    "assert had_l2_error < 1e-4\n",
    "assert 2e-2 < fp4_l2_error < 6e-2\n",
    "assert triton_l2_discrepancy < fp4_l2_error / 10\n",
    "\n",
    "had_grad_l2_error = (torch.linalg.norm(grad - y_had_grad) / torch.linalg.norm(grad)).pow(2).detach().item()\n",
    "fp4_grad_l2_error = (torch.linalg.norm(grad - y_fp4_grad) / torch.linalg.norm(grad)).pow(2).detach().item()\n",
    "triton_grad_l2_discrepancy = (torch.linalg.norm(y_fp4_grad - y_triton_grad) / torch.linalg.norm(grad)).pow(2).detach().item()\n",
    "\n",
    "print(f\"Hadamard grad L2 error: {had_grad_l2_error:.1e}\")\n",
    "print(f\"FP4 grad L2 error: {fp4_grad_l2_error:.1e}\")\n",
    "print(f\"Triton grad L2 discrepancy: {triton_grad_l2_discrepancy:.1e}\")\n",
    "\n",
    "assert had_grad_l2_error < 1e-4\n",
    "assert 6e-2 < fp4_grad_l2_error < 15e-2\n",
    "assert triton_grad_l2_discrepancy < fp4_grad_l2_error / 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemm_fns = {\n",
    "    \"baseline\": F.linear,\n",
    "    \"+hadamard\": HadamardGemm.apply,\n",
    "    \"+mxfp4\": MXFP4Gemm.apply,\n",
    "    \"+triton\": TritonGemm.apply,\n",
    "}\n",
    "\n",
    "gemm_compiled_fns = {\n",
    "    k: torch.compile(v) for k, v in gemm_fns.items()\n",
    "}\n",
    "\n",
    "\n",
    "def benchmark_gpu(fn, input_size, weight_size, num_iterations=100, warmup=10):\n",
    "    \"\"\"Benchmark a function on GPU\"\"\"\n",
    "    input = torch.randn(*input_size, device=\"cuda\", dtype=DTYPE, requires_grad=True)\n",
    "    weight = torch.randn(*weight_size, device=\"cuda\", dtype=DTYPE, requires_grad=True)\n",
    "\n",
    "    # # Warmup\n",
    "    # for _ in range(warmup):\n",
    "    #     result = fn(input, weight)\n",
    "\n",
    "    # torch.cuda.synchronize()\n",
    "\n",
    "    # # Measure forward pass\n",
    "    # start_time = time.time()\n",
    "    # for _ in range(num_iterations):\n",
    "    #     result = fn(input, weight)\n",
    "    #     torch.cuda.synchronize()\n",
    "    # forward_time = (time.time() - start_time) / num_iterations\n",
    "\n",
    "    # # Warmup\n",
    "    # grad = torch.randn_like(result)\n",
    "    # for _ in range(warmup):\n",
    "    #     result.backward(grad, retain_graph=True)\n",
    "\n",
    "    # # Measure backward pass\n",
    "    # start_time = time.time()\n",
    "    # for _ in range(num_iterations):\n",
    "    #     result = fn(input, weight)\n",
    "    #     result.backward(grad, retain_graph=True)\n",
    "    #     torch.cuda.synchronize()\n",
    "    # backward_time = (time.time() - start_time) / num_iterations - forward_time\n",
    "\n",
    "    # return {\n",
    "    #     \"forward_ms\": forward_time * 1000,\n",
    "    #     \"backward_ms\": backward_time * 1000,\n",
    "    #     \"total_ms\": (forward_time + backward_time) * 1000\n",
    "    # }\n",
    "    \n",
    "    result = fn(input, weight)\n",
    "    grad = torch.randn_like(result)\n",
    "    \n",
    "    forward_time = triton.testing.do_bench(\n",
    "        lambda: fn(input, weight), warmup=warmup, rep=num_iterations,\n",
    "    )\n",
    "    backward_time = triton.testing.do_bench(\n",
    "        lambda: result.backward(grad, retain_graph=True), warmup=warmup, rep=num_iterations,\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"forward_ms\": forward_time,\n",
    "        \"backward_ms\": backward_time,\n",
    "        \"total_ms\": (forward_time + backward_time)\n",
    "    }\n",
    "\n",
    "\n",
    "def run_gpu_benchmarks(batch_size=64, seq_len=512, hidden_size=1024):\n",
    "    \"\"\"Run benchmarks for different GEMM implementations on GPU\"\"\"\n",
    "    input_size = (batch_size, seq_len, hidden_size)\n",
    "    weight_size = (hidden_size, hidden_size)\n",
    "\n",
    "    results = {}\n",
    "    for name, fn in gemm_compiled_fns.items():\n",
    "        print(f\"Benchmarking {name}...\")\n",
    "        results[name] = benchmark_gpu(fn, input_size, weight_size)\n",
    "\n",
    "    # Print results\n",
    "    print(\"\\nGPU Benchmark Results (ms):\")\n",
    "    print(f\"{'Method':<15} {'Forward':<10} {'Backward':<10} {'Total':<10}\")\n",
    "    print(\"-\" * 45)\n",
    "    for name, timings in results.items():\n",
    "        forward = f\"{timings['forward_ms']:.2f}\"\n",
    "        backward = f\"{timings['backward_ms']:.2f}\" if timings['backward_ms'] is not None else \"N/A\"\n",
    "        total = f\"{timings['total_ms']:.2f}\"\n",
    "        print(f\"{name:<15} {forward:<10} {backward:<10} {total:<10}\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline...\n",
      "Benchmarking +hadamard...\n",
      "Benchmarking +mxfp4...\n",
      "Benchmarking +triton...\n",
      "\n",
      "GPU Benchmark Results (ms):\n",
      "Method          Forward    Backward   Total     \n",
      "---------------------------------------------\n",
      "baseline        5.15       11.36      16.51     \n",
      "+hadamard       6.34       13.62      19.96     \n",
      "+mxfp4          7.38       14.98      22.36     \n",
      "+triton         7.34       14.15      21.48     \n"
     ]
    }
   ],
   "source": [
    "_ = run_gpu_benchmarks(hidden_size=4096)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CUDA Kernels Needed\n",
    "\n",
    "---\n",
    "\n",
    "## Forward HT+Quant:\n",
    "\n",
    " * **Inputs**: \n",
    "    * `x`: torch.Tensor; shape=[M, K]; dtype=float32; already contiguous.\n",
    "    * `hadamard_matrix`: torch.Tensor; shape=[32, 32]; dtype=float32.\n",
    " * **Outputs**:\n",
    "    * `q`: torch.Tensor; shape=[M, K]; dtype=MXFP4 with scales along K;\n",
    "    * `mask`: torch.Tensor; shape=[M, K]; dtype=bool;\n",
    " * **Reference Impl**:\n",
    "    * Triton impl up to pseudoquant:\n",
    "        ```\n",
    "        q = mxfp4_forward_kernel_wrapper(\n",
    "            x,\n",
    "            hadamard_matrix,\n",
    "            stochastic_round=False,\n",
    "            quest=True,\n",
    "        )\n",
    "        ```\n",
    " * **Features**:\n",
    "    * Assume `M,K` divisible by 128.\n",
    "    * RTN projection.\n",
    "    * Scales based on STD.\n",
    "\n",
    "\n",
    "---\n",
    "    \n",
    "## Backward HT+Quant:\n",
    "\n",
    " * **Inputs**: \n",
    "    * `x`: torch.Tensor; shape=[M, K]; dtype=float32; already contiguous.\n",
    "    * `hadamard_matrix`: torch.Tensor; shape=[32, 32]; dtype=float32.\n",
    " * **Outputs**:\n",
    "    * `q`: torch.Tensor; shape=[M, K]; dtype=MXFP4 with scales along K;\n",
    " * **Reference Impl**:\n",
    "    * Triton impl up to pseudoquant:\n",
    "        ```\n",
    "        q = mxfp4_forward_kernel_wrapper(\n",
    "            x,\n",
    "            hadamard_matrix,\n",
    "            stochastic_round=True,\n",
    "            quest=False,\n",
    "        )\n",
    "        ```\n",
    " * **Features**:\n",
    "    * Assume `M,K` divisible by 128.\n",
    "    * Stochastic Rounding.\n",
    "    * Scales based on absmax.\n",
    "\n",
    "---\n",
    "\n",
    "## Backward Transpose+HT+Quant:\n",
    "\n",
    " * **Inputs**: \n",
    "    * `x`: torch.Tensor; shape=[M, K]; dtype=float32; already contiguous.\n",
    "    * `hadamard_matrix`: torch.Tensor; shape=[32, 32]; dtype=float32.\n",
    " * **Outputs**:\n",
    "    * `q`: torch.Tensor; shape=[K, M]; dtype=MXFP4 with scales along M;\n",
    " * **Reference Impl**:\n",
    "    * Triton impl up to pseudoquant:\n",
    "        ```\n",
    "        q = mxfp4_forward_kernel_wrapper(\n",
    "            x.T,\n",
    "            hadamard_matrix,\n",
    "            stochastic_round=True,\n",
    "            quest=False,\n",
    "        )\n",
    "        ```\n",
    " * **Features**:\n",
    "    * Assume `M,K` divisible by 128.\n",
    "    * Stochastic Rounding.\n",
    "    * Scales based on absmax.\n",
    "\n",
    "---\n",
    "    \n",
    "## Backward Dequant+Transpose+HT+Quant:\n",
    "\n",
    " * **Inputs**:\n",
    "    * `q`: torch.Tensor; shape=[M, K]; dtype=MXFP4 with scales along K;\n",
    "    * `hadamard_matrix`: torch.Tensor; shape=[32, 32]; dtype=float32.\n",
    " * **Outputs**:\n",
    "    * `qq`: torch.Tensor; shape=[K, M]; dtype=MXFP4 with scales along M;\n",
    " * **Reference Impl**:\n",
    "    * Triton impl up to pseudoquant:\n",
    "        ```\n",
    "        qq = mxfp4_forward_kernel_wrapper(\n",
    "            q.T,\n",
    "            hadamard_matrix,\n",
    "            stochastic_round=True,\n",
    "            quest=False,\n",
    "        )\n",
    "        ```\n",
    " * **Features**:\n",
    "    * Assume `M,K` divisible by 128.\n",
    "    * Stochastic Rounding.\n",
    "    * Scales based on absmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qutlass import matmul_mxf4_bf16_tn\n",
    "\n",
    "@torch.library.custom_op(\"quartet::matmul_mxf4_bf16_tn_op\", mutates_args=())\n",
    "def matmul_mxf4_bf16_tn_op(\n",
    "    x: torch.Tensor, w: torch.Tensor, xs: torch.Tensor, ws: torch.Tensor, alpha: torch.Tensor\n",
    ") -> torch.Tensor:\n",
    "    return matmul_mxf4_bf16_tn(\n",
    "        x.view(torch.uint8), w.view(torch.uint8), xs.view(torch.float8_e8m0fnu), ws.view(torch.float8_e8m0fnu), alpha\n",
    "    )\n",
    "\n",
    "@matmul_mxf4_bf16_tn_op.register_fake\n",
    "def _(x, w, xs, ws, alpha):\n",
    "    return x.new_empty(x.shape[0], w.shape[0], dtype=DTYPE)\n",
    "\n",
    "\n",
    "from qutlass import fusedQuantizeMx\n",
    "\n",
    "@torch.library.custom_op(\"quartet::fusedQuantizeMx_op\", mutates_args=())\n",
    "def fusedQuantizeMx_op(\n",
    "    x_flat: torch.Tensor, hadamard_matrix: torch.Tensor, return_mask: bool\n",
    ") -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    if return_mask:\n",
    "        return fusedQuantizeMx(x_flat, hadamard_matrix, return_mask=True)\n",
    "    else:\n",
    "        return fusedQuantizeMx(x_flat, hadamard_matrix, return_mask=False) + (None,)\n",
    "\n",
    "@fusedQuantizeMx_op.register_fake\n",
    "def _(x_flat, hadamard_matrix, return_mask):\n",
    "    rows, cols = x_flat.shape[0], x_flat.shape[1] // 32\n",
    "    padded_rows = ((rows + 128 - 1) // 128) * 128\n",
    "    padded_cols = ((cols + 4 - 1) // 4) * 4\n",
    "\n",
    "    xh_e2m1 = torch.empty(\n",
    "        x_flat.shape[0], x_flat.shape[1] // 2, dtype=torch.uint8, device=x_flat.device\n",
    "    )\n",
    "    xh_e8m0 = torch.empty(\n",
    "        padded_rows, padded_cols, dtype=torch.uint8, device=x_flat.device\n",
    "    )\n",
    "    clip_mask = torch.empty(*x_flat.shape[:-1], x_flat.size(-1) // 8,  dtype=torch.uint8, device=x_flat.device) if return_mask else None\n",
    "    return xh_e2m1, xh_e8m0, clip_mask\n",
    "\n",
    "\n",
    "from qutlass import backward_t_bf16\n",
    "\n",
    "@torch.library.custom_op(\"quartet::backward_t_bf16_op\", mutates_args=())\n",
    "def backward_t_bf16_op(\n",
    "    grad_output_flat: torch.Tensor, hadamard_matrix: torch.Tensor\n",
    ") -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    return backward_t_bf16(grad_output_flat, hadamard_matrix)\n",
    "\n",
    "@backward_t_bf16_op.register_fake\n",
    "def _(grad_output_flat, hadamard_matrix):\n",
    "    xh_e2m1 = torch.empty(grad_output_flat.shape[1], grad_output_flat.shape[0] // 2,  dtype=torch.uint8, device=grad_output_flat.device)\n",
    "    xh_e8m0 = torch.empty(grad_output_flat.shape[1], grad_output_flat.shape[0] // 32, dtype=torch.uint8, device=grad_output_flat.device)\n",
    "\n",
    "    return xh_e2m1, xh_e8m0\n",
    "\n",
    "\n",
    "from qutlass import backward_qt_bf16\n",
    "\n",
    "@torch.library.custom_op(\"quartet::backward_qt_bf16_op\", mutates_args=())\n",
    "def backward_qt_bf16_op(\n",
    "    x_e2m1: torch.Tensor,\n",
    "    x_e8m0: torch.Tensor,\n",
    "    h: torch.Tensor,\n",
    "    alpha: torch.Tensor,\n",
    ") -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    assert x_e2m1.dim() == 2\n",
    "    return backward_qt_bf16(x_e2m1, x_e8m0, h, alpha)\n",
    "\n",
    "@backward_qt_bf16_op.register_fake\n",
    "def _(x_e2m1, x_e8m0, h, alpha):\n",
    "    assert x_e2m1.dim() == 2\n",
    "    xh_e2m1 = torch.empty(x_e2m1.shape[1] * 2, x_e2m1.shape[0] // 2, dtype=torch.uint8, device=h.device)\n",
    "    xh_e8m0 = torch.empty(x_e8m0.shape[1] * 32, x_e8m0.shape[0] // 32, dtype=torch.uint8, device=h.device)\n",
    "    return xh_e2m1, xh_e8m0\n",
    "\n",
    "\n",
    "from qutlass import matmul_mxf8_bf16_tn\n",
    "\n",
    "@torch.library.custom_op(\"quartet::matmul_mxf8_bf16_tn_op\", mutates_args=())\n",
    "def matmul_mxf8_bf16_tn_op(\n",
    "    x: torch.Tensor, w: torch.Tensor, xs: torch.Tensor, ws: torch.Tensor, alpha: torch.Tensor\n",
    ") -> torch.Tensor:\n",
    "    return matmul_mxf8_bf16_tn(\n",
    "        x, w, xs.view(torch.float8_e8m0fnu), ws.view(torch.float8_e8m0fnu), alpha\n",
    "    )\n",
    "\n",
    "@matmul_mxf8_bf16_tn_op.register_fake\n",
    "def _(x, w, xs, ws, alpha):\n",
    "    return x.new_empty(x.shape[0], w.shape[0], dtype=DTYPE)\n",
    "\n",
    "\n",
    "from qutlass.utils import to_blocked\n",
    "\n",
    "def _unpack_mask(clip_mask: torch.Tensor) -> torch.Tensor:\n",
    "    clip_mask_unpacked_dq = torch.zeros(*clip_mask.shape[:-1], clip_mask.size(-1) * 8, dtype=torch.bool, device=clip_mask.device)\n",
    "    for i in range(8):\n",
    "        clip_mask_unpacked_dq[..., i::8] = (clip_mask >> i) & 1\n",
    "    return clip_mask_unpacked_dq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHA_FWD = torch.tensor(1., device=\"cuda\")\n",
    "ALPHA_BWD = torch.tensor(1./9., device=\"cuda\")\n",
    "\n",
    "class CudaGemm(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, weight, deterministic=True):\n",
    "        ctx.batch = input.shape[0]\n",
    "        ctx.seq = input.shape[1]\n",
    "        ctx.in_dim = weight.shape[1]\n",
    "        ctx.out_dim = weight.shape[0]\n",
    "\n",
    "        ctx.deterministic = deterministic\n",
    "\n",
    "        input_hf_e2m1, input_hf_e8m0, input_hf_mask = fusedQuantizeMx_op(\n",
    "            input.flatten(end_dim=-2),\n",
    "            FORWARD_HADAMARD_MATRIX,\n",
    "            return_mask=input.requires_grad,\n",
    "        )\n",
    "\n",
    "        weight_hf_e2m1, weight_hf_e8m0, weight_hf_mask = fusedQuantizeMx_op(\n",
    "            weight,\n",
    "            FORWARD_HADAMARD_MATRIX,\n",
    "            return_mask=input.requires_grad,\n",
    "        )\n",
    "\n",
    "        ctx.save_for_backward(input_hf_e2m1, input_hf_e8m0, input_hf_mask, weight_hf_e2m1, weight_hf_e8m0, weight_hf_mask)\n",
    "\n",
    "        input_hf_scale_block = to_blocked(input_hf_e8m0, False)\n",
    "        weight_hf_scale_block = to_blocked(weight_hf_e8m0, False)\n",
    "\n",
    "        out = matmul_mxf4_bf16_tn_op(\n",
    "            input_hf_e2m1,\n",
    "            weight_hf_e2m1,\n",
    "            input_hf_scale_block,\n",
    "            weight_hf_scale_block,\n",
    "            ALPHA_FWD,\n",
    "        )\n",
    "        return out.view(*input.shape[:-1], weight.size(-2))\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        global BACKWARD_HADAMARD_MATRIX\n",
    "        input_hf_e2m1, input_hf_e8m0, input_hf_mask, weight_hf_e2m1, weight_hf_e8m0, weight_hf_mask = ctx.saved_tensors\n",
    "\n",
    "        if not ctx.deterministic:\n",
    "            BACKWARD_HADAMARD_MATRIX = BACKWARD_HADAMARD_MATRIX * (\n",
    "                torch.randint(0, 2, (32,), device=BACKWARD_HADAMARD_MATRIX.device, dtype=BACKWARD_HADAMARD_MATRIX.dtype)\n",
    "                * 2. - 1.\n",
    "            )\n",
    "\n",
    "        grad_output_hb_e2m1, grad_output_hb_e8m0, _ = fusedQuantizeMx_op(\n",
    "            grad_output.flatten(end_dim=-2),\n",
    "            BACKWARD_HADAMARD_MATRIX,\n",
    "            False,\n",
    "        )\n",
    "\n",
    "        hft_weightt_hb_e2m1, hft_weightt_hb_e8m0 = backward_qt_bf16_op(weight_hf_e2m1, weight_hf_e8m0, BACKWARD_HADAMARD_MATRIX, ALPHA_FWD)\n",
    "        grad_output_hb_scale_block = to_blocked(grad_output_hb_e8m0, False)\n",
    "        hft_weightt_hb_scale_block = to_blocked(hft_weightt_hb_e8m0, False)\n",
    "        grad_input_hf = matmul_mxf4_bf16_tn_op(\n",
    "            grad_output_hb_e2m1,\n",
    "            hft_weightt_hb_e2m1,\n",
    "            grad_output_hb_scale_block,\n",
    "            hft_weightt_hb_scale_block,\n",
    "            ALPHA_BWD,\n",
    "        )\n",
    "\n",
    "        input_mask_hf = _unpack_mask(input_hf_mask)\n",
    "        grad_input = (\n",
    "            (grad_input_hf.view(-1, 32) * input_mask_hf.view(-1, 32).to(grad_input_hf.dtype))\n",
    "            @ FORWARD_HADAMARD_MATRIX.T\n",
    "        ).view(*grad_output.shape[:-1], weight_hf_e2m1.size(-1) * 2)\n",
    "\n",
    "        grad_outputt_hb_e2m1, grad_outputt_hb_e8m0 = backward_t_bf16_op(grad_output.flatten(end_dim=-2), BACKWARD_HADAMARD_MATRIX)\n",
    "        hft_inputt_hb_e2m1, hft_inputt_hb_e8m0 = backward_qt_bf16_op(input_hf_e2m1, input_hf_e8m0, BACKWARD_HADAMARD_MATRIX, ALPHA_FWD)\n",
    "        grad_outputt_hb_scale_block = to_blocked(grad_outputt_hb_e8m0, False)\n",
    "        hft_inputt_hb_scale_block = to_blocked(hft_inputt_hb_e8m0, False)\n",
    "        grad_weight_hf = matmul_mxf4_bf16_tn_op(\n",
    "            grad_outputt_hb_e2m1,\n",
    "            hft_inputt_hb_e2m1,\n",
    "            grad_outputt_hb_scale_block,\n",
    "            hft_inputt_hb_scale_block,\n",
    "            ALPHA_BWD,\n",
    "        )\n",
    "        \n",
    "        # torch._assert(grad_weight_hf.shape == (weight_hf_e2m1.size(0), weight_hf_e2m1.size(1) * 2), f\"{grad_outputt_hb_e2m1.shape=} {hft_inputt_hb_e2m1.shape=} {grad_weight_hf.shape=} {weight_hf_e2m1.shape=}\")\n",
    "\n",
    "        weight_mask_hf = _unpack_mask(weight_hf_mask)\n",
    "        grad_weight = (\n",
    "            (grad_weight_hf.view(-1, 32) * weight_mask_hf.view(-1, 32).to(grad_weight_hf.dtype))\n",
    "            @ FORWARD_HADAMARD_MATRIX.T\n",
    "        ).view(grad_output.size(-1), weight_hf_e2m1.size(-1) * 2)\n",
    "        return grad_input, grad_weight, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "DUMMY_E8M0 = torch.ones(2 ** 30, dtype=torch.float8_e8m0fnu, device=\"cuda\").view(torch.uint8)\n",
    "\n",
    "class Fp8Gemm(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def get_dummy_e8m0(x_e4m3: torch.Tensor) -> torch.Tensor:\n",
    "        x_e8m0 = DUMMY_E8M0[:x_e4m3.numel() // 32].view(*x_e4m3.shape[:-1], x_e4m3.size(-1) // 32)\n",
    "        return x_e8m0\n",
    "\n",
    "    @staticmethod\n",
    "    def mm_fp8(a_e4m3: torch.Tensor, b_e4m3: torch.Tensor) -> torch.Tensor:\n",
    "        c_bf16 = matmul_mxf8_bf16_tn_op(a_e4m3, b_e4m3, Fp8Gemm.get_dummy_e8m0(a_e4m3), Fp8Gemm.get_dummy_e8m0(b_e4m3), ALPHA_FWD)\n",
    "        return c_bf16\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, weight):\n",
    "        input_e4m3 = input.flatten(end_dim=-2).to(dtype=torch.float8_e4m3fn)\n",
    "        weight_e4m3 = weight.to(dtype=torch.float8_e4m3fn)\n",
    "        ctx.save_for_backward(input_e4m3, weight_e4m3)\n",
    "\n",
    "        return Fp8Gemm.mm_fp8(\n",
    "            input_e4m3,\n",
    "            weight_e4m3,\n",
    "        ).view(*input.shape[:-1], weight.size(-2))\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input_e4m3, weight_e4m3 = ctx.saved_tensors\n",
    "\n",
    "        grad_output_e4m3 = grad_output.flatten(end_dim=-2).to(dtype=torch.float8_e4m3fn)\n",
    "        \n",
    "        grad_input = Fp8Gemm.mm_fp8(\n",
    "            grad_output_e4m3,\n",
    "            weight_e4m3.T.contiguous(),\n",
    "        ).view(*grad_output.shape[:-1], weight_e4m3.size(-1))\n",
    "\n",
    "        grad_outputt_e4m3 = grad_output.flatten(end_dim=-2).to(dtype=torch.float8_e4m3fn)\n",
    "        grad_weight = Fp8Gemm.mm_fp8(\n",
    "            grad_outputt_e4m3.T.contiguous(),\n",
    "            input_e4m3.T.contiguous(),\n",
    "        ).view(grad_output.size(-1), weight_e4m3.size(-1))\n",
    "\n",
    "        return grad_input, grad_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_grad_enabled(True)\n",
    "\n",
    "y_cuda = CudaGemm.apply(x, w, DETERMINISTIC_FOR_TESTS)\n",
    "y_cuda.backward(y_grad)\n",
    "y_cuda_grad = w.grad.clone()\n",
    "w.grad = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_fp8 = Fp8Gemm.apply(x, w)\n",
    "y_fp8.backward(y_grad)\n",
    "y_fp8_grad = w.grad.clone()\n",
    "w.grad = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hadamard L2 error: 1.5e-05\n",
      "FP4 L2 error: 5.7e-02\n",
      "FP8 L2 error: 1.4e-03\n",
      "Triton L2 discrepancy: 1.5e-03\n",
      "Cuda L2 discrepancy: 1.5e-03\n",
      "Hadamard grad L2 error: 1.6e-05\n",
      "FP4 grad L2 error: 1.2e-01\n",
      "FP8 grad L2 error: 1.4e-03\n",
      "Triton grad L2 discrepancy: 4.5e-03\n",
      "Cuda grad L2 discrepancy: 4.5e-03\n"
     ]
    }
   ],
   "source": [
    "had_l2_error = (torch.linalg.norm(y - y_had) / torch.linalg.norm(y)).pow(2).detach().item()\n",
    "fp4_l2_error = (torch.linalg.norm(y - y_fp4) / torch.linalg.norm(y)).pow(2).detach().item()\n",
    "fp8_l2_error = (torch.linalg.norm(y - y_fp8) / torch.linalg.norm(y)).pow(2).detach().item()\n",
    "triton_l2_discrepancy = (torch.linalg.norm(y_fp4 - y_triton) / torch.linalg.norm(y)).pow(2).detach().item()\n",
    "cuda_l2_discrepancy = (torch.linalg.norm(y_fp4 - y_cuda) / torch.linalg.norm(y)).pow(2).detach().item()\n",
    "\n",
    "print(f\"Hadamard L2 error: {had_l2_error:.1e}\")\n",
    "print(f\"FP4 L2 error: {fp4_l2_error:.1e}\")\n",
    "print(f\"FP8 L2 error: {fp8_l2_error:.1e}\")\n",
    "print(f\"Triton L2 discrepancy: {triton_l2_discrepancy:.1e}\")\n",
    "print(f\"Cuda L2 discrepancy: {cuda_l2_discrepancy:.1e}\")\n",
    "assert had_l2_error < 1e-4\n",
    "assert fp8_l2_error < 2e-2 < fp4_l2_error < 6e-2\n",
    "assert triton_l2_discrepancy < fp4_l2_error / 10\n",
    "assert cuda_l2_discrepancy < fp4_l2_error / 10\n",
    "\n",
    "had_grad_l2_error = (torch.linalg.norm(grad - y_had_grad) / torch.linalg.norm(grad)).pow(2).detach().item()\n",
    "fp4_grad_l2_error = (torch.linalg.norm(grad - y_fp4_grad) / torch.linalg.norm(grad)).pow(2).detach().item()\n",
    "fp8_grad_l2_error = (torch.linalg.norm(grad - y_fp8_grad) / torch.linalg.norm(grad)).pow(2).detach().item()\n",
    "triton_grad_l2_discrepancy = (torch.linalg.norm(y_fp4_grad - y_triton_grad) / torch.linalg.norm(grad)).pow(2).detach().item()\n",
    "cuda_grad_l2_discrepancy = (torch.linalg.norm(y_fp4_grad - y_cuda_grad) / torch.linalg.norm(grad)).pow(2).detach().item()\n",
    "\n",
    "print(f\"Hadamard grad L2 error: {had_grad_l2_error:.1e}\")\n",
    "print(f\"FP4 grad L2 error: {fp4_grad_l2_error:.1e}\")\n",
    "print(f\"FP8 grad L2 error: {fp8_grad_l2_error:.1e}\")\n",
    "print(f\"Triton grad L2 discrepancy: {triton_grad_l2_discrepancy:.1e}\")\n",
    "print(f\"Cuda grad L2 discrepancy: {cuda_grad_l2_discrepancy:.1e}\")\n",
    "\n",
    "assert had_grad_l2_error < 1e-4\n",
    "assert fp8_grad_l2_error < 6e-2 < fp4_grad_l2_error < 15e-2\n",
    "assert triton_grad_l2_discrepancy < fp4_grad_l2_error / 10\n",
    "assert cuda_grad_l2_discrepancy < fp4_grad_l2_error / 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemm_fns = {\n",
    "    \"baseline\": F.linear,\n",
    "    # \"+hadamard\": HadamardGemm.apply,\n",
    "    # \"+mxfp4\": MXFP4Gemm.apply,\n",
    "    # \"+triton\": TritonGemm.apply,\n",
    "    \"+cuda\": CudaGemm.apply,\n",
    "    \"+fp8\": Fp8Gemm.apply,\n",
    "}\n",
    "\n",
    "torch._dynamo.config.compiled_autograd = True\n",
    "compile_kwargs = {\"fullgraph\": True}\n",
    "\n",
    "\n",
    "def benchmark_gpu(fn, input_size, weight_size, num_iterations=100, warmup=10):\n",
    "    \"\"\"Benchmark a function on GPU\"\"\"\n",
    "    input = torch.randn(*input_size, device=\"cuda\", dtype=DTYPE, requires_grad=True)\n",
    "    weight = torch.randn(*weight_size, device=\"cuda\", dtype=DTYPE, requires_grad=True)\n",
    "\n",
    "\n",
    "    # Forward\n",
    "    torch.set_grad_enabled(False)\n",
    "    \n",
    "    compiled_forward_fn = torch.compile(fn, **compile_kwargs)\n",
    "    \n",
    "    ms = triton.testing.do_bench(\n",
    "        lambda: compiled_forward_fn(input, weight), warmup=warmup, rep=num_iterations,\n",
    "    )\n",
    "    forward_time = ms\n",
    "\n",
    "    # Forward+Backward\n",
    "    grad = torch.randn_like(fn(input, weight))\n",
    "    torch.set_grad_enabled(True)    \n",
    "    \n",
    "    def compiled_forward_backward(input, weight, grad):\n",
    "        with torch._dynamo.compiled_autograd._enable(torch.compile(**compile_kwargs)):\n",
    "        # with torch._dynamo.utils.maybe_enable_compiled_autograd(True, dynamic=False, **compile_kwargs):\n",
    "            output = fn(input, weight)\n",
    "            output.backward(grad)\n",
    "    \n",
    "    ms = triton.testing.do_bench(\n",
    "        lambda: compiled_forward_backward(input, weight, grad), warmup=warmup, rep=num_iterations,\n",
    "    )\n",
    "    total_time = ms\n",
    "\n",
    "    return {\n",
    "        \"forward_ms\": forward_time,\n",
    "        \"total_ms\": total_time,\n",
    "    }\n",
    "\n",
    "\n",
    "def run_gpu_benchmarks(batch_size=64, seq_len=512, hidden_size=1024):\n",
    "    \"\"\"Run benchmarks for different GEMM implementations on GPU\"\"\"\n",
    "    input_size = (batch_size, seq_len, hidden_size)\n",
    "    weight_size = (hidden_size, hidden_size)\n",
    "\n",
    "    results = {}\n",
    "    for name, fn in gemm_fns.items():\n",
    "        print(f\"Benchmarking {name}...\")\n",
    "        results[name] = benchmark_gpu(fn, input_size, weight_size)\n",
    "\n",
    "    # Print results\n",
    "    print(\"\\nGPU Benchmark Results (ms):\")\n",
    "    print(f\"{'Method':<15} {'Forward':<10} {'Total':<10}\")\n",
    "    print(\"-\" * 45)\n",
    "    for name, timings in results.items():\n",
    "        forward = f\"{timings['forward_ms']:.2f}\"\n",
    "        total = f\"{timings['total_ms']:.2f}\"\n",
    "        print(f\"{name:<15} {forward:<10} {total:<10}\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline...\n",
      "Benchmarking +cuda...\n",
      "Benchmarking +fp8...\n",
      "\n",
      "GPU Benchmark Results (ms):\n",
      "Method          Forward    Total     \n",
      "---------------------------------------------\n",
      "baseline        0.38       1.25      \n",
      "+cuda           0.15       1.42      \n",
      "+fp8            0.21       0.89      \n"
     ]
    }
   ],
   "source": [
    "_ = run_gpu_benchmarks(hidden_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shapes = {\n",
    "#     # Q K V Down Up Gate Down\n",
    "#     \"100M\": [(1024 * 3, 1024), (1024, 1024), (2816 * 2, 1024), (1024, 2816)],\n",
    "#     \"800M\": [(2048 * 3, 2048), (2048, 2048), (5632 * 2, 2048), (2048, 5632)],\n",
    "#     \"3B\": [(3072 * 3, 3072), (3072, 3072), (8192 * 2, 3072), (3072, 8192)],\n",
    "#     \"7B\": [(4096 * 3, 4096), (4096, 4096), (11008 * 2, 4096), (4096, 11008)],\n",
    "#     \"22B\": [(6144 * 3, 6144), (6144, 6144), (16384 * 2, 6144), (6144, 16384)],\n",
    "#     \"52B\": [(8192 * 3, 8192), (8192, 8192), (22016 * 2, 8192), (8192, 22016)],\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "def from_num_head(n_head):\n",
    "    h = n_head * 128\n",
    "    inter = h * 8 / 3\n",
    "    inter = int((inter - 1) // 256) * 256 + 256\n",
    "    \n",
    "    shapes = [(h * 3, h), (h, h), (inter * 2, h), (h, inter)]\n",
    "    assert sum(map(lambda x: x[0] * x[1], shapes)) == 4 * h**2 + 3 * h * inter\n",
    "    \n",
    "    return (4 * h**2 + 3 * h * inter) * n_head, shapes\n",
    "\n",
    "\n",
    "shapes = {\n",
    "    n: shapes for (n, shapes) in map(from_num_head, chain(range(8, 13), range(16, 65, 4)))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS = {}\n",
    "\n",
    "def run_gpu_benchmarks_layer(batch_size=64, seq_len=512):\n",
    "    all_shapes = set()\n",
    "    for shape_list in shapes.values():\n",
    "        all_shapes.update(shape_list)\n",
    "    all_shapes = sorted(all_shapes)\n",
    "\n",
    "    for name in ['baseline', '+cuda', '+fp8']:\n",
    "        print(f\"Benchmarking {name}...\")\n",
    "        fn = gemm_fns[name]\n",
    "        if name not in RESULTS:\n",
    "            RESULTS[name] = {}\n",
    "        for weight_size in tqdm(all_shapes):\n",
    "            input_size = (batch_size, seq_len, weight_size[1])\n",
    "            if weight_size not in RESULTS[name]:\n",
    "                print(f\"Benchmarking {name} {weight_size}...\")\n",
    "                RESULTS[name][weight_size] = benchmark_gpu(fn, input_size, weight_size)\n",
    "            else:\n",
    "                print(f\"Skipping {name} {weight_size} because it already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/72 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (1024, 1024)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|         | 1/72 [00:00<00:16,  4.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (1024, 2816)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|         | 2/72 [00:01<01:16,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (1152, 1152)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|         | 3/72 [00:03<01:33,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (1152, 3072)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|         | 4/72 [00:03<01:02,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (1280, 1280)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|         | 5/72 [00:04<00:44,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (1280, 3584)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|         | 6/72 [00:04<00:34,  1.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (1408, 1408)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|         | 7/72 [00:04<00:28,  2.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (1408, 3840)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|         | 8/72 [00:04<00:24,  2.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (1536, 1536)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|        | 9/72 [00:05<00:20,  3.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (1536, 4096)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|        | 10/72 [00:05<00:19,  3.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (2048, 2048)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|        | 11/72 [00:05<00:17,  3.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (2048, 5632)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|        | 12/72 [00:05<00:17,  3.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (2560, 2560)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|        | 13/72 [00:06<00:16,  3.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (2560, 6912)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|        | 14/72 [00:06<00:17,  3.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (3072, 1024)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|        | 15/72 [00:06<00:16,  3.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (3072, 3072)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|       | 16/72 [00:07<00:16,  3.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (3072, 8192)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|       | 17/72 [00:07<00:17,  3.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (3456, 1152)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|       | 18/72 [00:07<00:16,  3.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (3584, 3584)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|       | 19/72 [00:07<00:15,  3.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (3584, 9728)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|       | 20/72 [00:08<00:18,  2.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (3840, 1280)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|       | 21/72 [00:08<00:16,  3.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (4096, 4096)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|       | 22/72 [00:09<00:16,  3.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (4096, 11008)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|      | 23/72 [00:09<00:19,  2.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (4224, 1408)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|      | 24/72 [00:09<00:16,  2.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (4608, 1536)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|      | 25/72 [00:10<00:15,  3.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (4608, 4608)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|      | 26/72 [00:10<00:15,  2.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (4608, 12288)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|      | 27/72 [00:11<00:19,  2.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (5120, 5120)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|      | 28/72 [00:11<00:18,  2.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (5120, 13824)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|      | 29/72 [00:12<00:22,  1.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (5632, 1024)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|     | 30/72 [00:12<00:18,  2.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (5632, 5632)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|     | 31/72 [00:12<00:18,  2.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (5632, 15104)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|     | 32/72 [00:13<00:22,  1.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (6144, 1152)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|     | 33/72 [00:14<00:18,  2.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (6144, 2048)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|     | 34/72 [00:14<00:16,  2.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (6144, 6144)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|     | 35/72 [00:14<00:16,  2.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (6144, 16384)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|     | 36/72 [00:15<00:22,  1.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (6656, 6656)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|    | 37/72 [00:16<00:20,  1.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (6656, 17920)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|    | 38/72 [00:17<00:26,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (7168, 1280)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|    | 39/72 [00:17<00:20,  1.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (7168, 7168)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|    | 40/72 [00:18<00:19,  1.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (7168, 19200)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|    | 41/72 [00:19<00:25,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (7680, 1408)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|    | 42/72 [00:20<00:20,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (7680, 2560)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|    | 43/72 [00:20<00:16,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (7680, 7680)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|    | 44/72 [00:21<00:16,  1.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (7680, 20480)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|   | 45/72 [00:22<00:23,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (8192, 1536)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|   | 46/72 [00:22<00:18,  1.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (8192, 8192)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|   | 47/72 [00:23<00:17,  1.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (8192, 22016)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|   | 48/72 [00:25<00:24,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (9216, 3072)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|   | 49/72 [00:25<00:19,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (10752, 3584)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|   | 50/72 [00:26<00:16,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (11264, 2048)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|   | 51/72 [00:26<00:13,  1.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (12288, 4096)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|  | 52/72 [00:27<00:12,  1.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (13824, 2560)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|  | 53/72 [00:27<00:11,  1.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (13824, 4608)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|  | 54/72 [00:28<00:10,  1.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (15360, 5120)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|  | 55/72 [00:29<00:11,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (16384, 3072)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|  | 56/72 [00:29<00:10,  1.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (16896, 5632)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|  | 57/72 [00:30<00:10,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (18432, 6144)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|  | 58/72 [00:31<00:11,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (19456, 3584)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%| | 59/72 [00:32<00:10,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (19968, 6656)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%| | 60/72 [00:33<00:11,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (21504, 7168)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%| | 61/72 [00:35<00:12,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (22016, 4096)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%| | 62/72 [00:36<00:10,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (23040, 7680)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%| | 63/72 [00:37<00:11,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (24576, 4608)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%| | 64/72 [00:39<00:09,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (24576, 8192)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%| | 65/72 [00:40<00:09,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (27648, 5120)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|| 66/72 [00:42<00:08,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (30208, 5632)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|| 67/72 [00:43<00:07,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (32768, 6144)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|| 68/72 [00:45<00:06,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (35840, 6656)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|| 69/72 [00:48<00:05,  1.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (38400, 7168)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|| 70/72 [00:50<00:04,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (40960, 7680)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|| 71/72 [00:53<00:02,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking baseline (44032, 8192)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 72/72 [00:56<00:00,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/72 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (1024, 1024)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|         | 1/72 [00:00<00:15,  4.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (1024, 2816)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|         | 2/72 [00:03<02:40,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (1152, 1152)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|         | 3/72 [00:13<06:40,  5.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (1152, 3072)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|         | 4/72 [00:14<04:05,  3.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (1280, 1280)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|         | 5/72 [00:14<02:40,  2.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (1280, 3584)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|         | 6/72 [00:14<01:49,  1.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (1408, 1408)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|         | 7/72 [00:14<01:17,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (1408, 3840)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|         | 8/72 [00:15<00:57,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (1536, 1536)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|        | 9/72 [00:15<00:43,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (1536, 4096)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|        | 10/72 [00:15<00:34,  1.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (2048, 2048)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|        | 11/72 [00:15<00:27,  2.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (2048, 5632)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|        | 12/72 [00:16<00:23,  2.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (2560, 2560)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|        | 13/72 [00:16<00:20,  2.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (2560, 6912)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|        | 14/72 [00:16<00:19,  3.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (3072, 1024)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|        | 15/72 [00:16<00:17,  3.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (3072, 3072)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|       | 16/72 [00:17<00:15,  3.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (3072, 8192)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|       | 17/72 [00:17<00:15,  3.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (3456, 1152)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|       | 18/72 [00:17<00:14,  3.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (3584, 3584)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|       | 19/72 [00:17<00:14,  3.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (3584, 9728)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|       | 20/72 [00:18<00:15,  3.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (3840, 1280)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|       | 21/72 [00:18<00:13,  3.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (4096, 4096)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|       | 22/72 [00:18<00:13,  3.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (4096, 11008)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|      | 23/72 [00:19<00:14,  3.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (4224, 1408)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|      | 24/72 [00:19<00:13,  3.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (4608, 1536)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|      | 25/72 [00:19<00:12,  3.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (4608, 4608)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|      | 26/72 [00:19<00:12,  3.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (4608, 12288)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|      | 27/72 [00:20<00:13,  3.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (5120, 5120)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|      | 28/72 [00:20<00:13,  3.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (5120, 13824)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|      | 29/72 [00:20<00:14,  3.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (5632, 1024)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|     | 30/72 [00:21<00:12,  3.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (5632, 5632)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|     | 31/72 [00:21<00:12,  3.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (5632, 15104)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|     | 32/72 [00:21<00:13,  2.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (6144, 1152)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|     | 33/72 [00:22<00:12,  3.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (6144, 2048)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|     | 34/72 [00:22<00:11,  3.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (6144, 6144)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|     | 35/72 [00:22<00:11,  3.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (6144, 16384)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|     | 36/72 [00:23<00:12,  2.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (6656, 6656)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|    | 37/72 [00:23<00:12,  2.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (6656, 17920)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|    | 38/72 [00:24<00:13,  2.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (7168, 1280)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|    | 39/72 [00:24<00:11,  2.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (7168, 7168)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|    | 40/72 [00:24<00:11,  2.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (7168, 19200)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|    | 41/72 [00:25<00:13,  2.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (7680, 1408)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|    | 42/72 [00:25<00:11,  2.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (7680, 2560)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|    | 43/72 [00:25<00:09,  2.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (7680, 7680)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|    | 44/72 [00:26<00:09,  2.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (7680, 20480)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|   | 45/72 [00:26<00:11,  2.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (8192, 1536)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|   | 46/72 [00:27<00:09,  2.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (8192, 8192)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|   | 47/72 [00:27<00:09,  2.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (8192, 22016)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|   | 48/72 [00:28<00:11,  2.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (9216, 3072)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|   | 49/72 [00:28<00:09,  2.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (10752, 3584)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|   | 50/72 [00:28<00:08,  2.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (11264, 2048)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|   | 51/72 [00:28<00:07,  2.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (12288, 4096)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|  | 52/72 [00:29<00:07,  2.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (13824, 2560)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|  | 53/72 [00:29<00:06,  2.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (13824, 4608)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|  | 54/72 [00:30<00:06,  2.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (15360, 5120)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|  | 55/72 [00:30<00:06,  2.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (16384, 3072)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|  | 56/72 [00:30<00:05,  2.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (16896, 5632)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|  | 57/72 [00:31<00:05,  2.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (18432, 6144)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|  | 58/72 [00:31<00:05,  2.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (19456, 3584)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%| | 59/72 [00:32<00:05,  2.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (19968, 6656)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%| | 60/72 [00:32<00:05,  2.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (21504, 7168)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%| | 61/72 [00:33<00:05,  2.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (22016, 4096)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%| | 62/72 [00:33<00:04,  2.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (23040, 7680)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%| | 63/72 [00:34<00:04,  1.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (24576, 4608)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%| | 64/72 [00:34<00:03,  2.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (24576, 8192)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%| | 65/72 [00:35<00:03,  1.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (27648, 5120)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|| 66/72 [00:35<00:03,  1.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (30208, 5632)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|| 67/72 [00:36<00:02,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (32768, 6144)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|| 68/72 [00:37<00:02,  1.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (35840, 6656)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|| 69/72 [00:37<00:01,  1.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (38400, 7168)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|| 70/72 [00:38<00:01,  1.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (40960, 7680)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|| 71/72 [00:39<00:00,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +cuda (44032, 8192)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 72/72 [00:40<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/72 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (1024, 1024)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|         | 1/72 [00:00<00:15,  4.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (1024, 2816)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|         | 2/72 [00:00<00:28,  2.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (1152, 1152)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|         | 3/72 [00:02<01:03,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (1152, 3072)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|         | 4/72 [00:02<00:44,  1.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (1280, 1280)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|         | 5/72 [00:02<00:33,  2.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (1280, 3584)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|         | 6/72 [00:02<00:26,  2.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (1408, 1408)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|         | 7/72 [00:03<00:22,  2.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (1408, 3840)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|         | 8/72 [00:03<00:19,  3.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (1536, 1536)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|        | 9/72 [00:03<00:17,  3.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (1536, 4096)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|        | 10/72 [00:03<00:16,  3.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (2048, 2048)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|        | 11/72 [00:04<00:15,  3.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (2048, 5632)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|        | 12/72 [00:04<00:15,  3.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (2560, 2560)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|        | 13/72 [00:04<00:14,  3.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (2560, 6912)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|        | 14/72 [00:04<00:15,  3.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (3072, 1024)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|        | 15/72 [00:05<00:14,  3.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (3072, 3072)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|       | 16/72 [00:05<00:14,  3.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (3072, 8192)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|       | 17/72 [00:05<00:14,  3.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (3456, 1152)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|       | 18/72 [00:05<00:13,  3.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (3584, 3584)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|       | 19/72 [00:06<00:13,  3.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (3584, 9728)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|       | 20/72 [00:06<00:14,  3.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (3840, 1280)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|       | 21/72 [00:06<00:13,  3.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (4096, 4096)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|       | 22/72 [00:07<00:13,  3.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (4096, 11008)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|      | 23/72 [00:07<00:14,  3.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (4224, 1408)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|      | 24/72 [00:07<00:13,  3.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (4608, 1536)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|      | 25/72 [00:07<00:12,  3.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (4608, 4608)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|      | 26/72 [00:08<00:12,  3.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (4608, 12288)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|      | 27/72 [00:08<00:14,  3.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (5120, 5120)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|      | 28/72 [00:08<00:13,  3.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (5120, 13824)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|      | 29/72 [00:09<00:15,  2.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (5632, 1024)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|     | 30/72 [00:09<00:13,  3.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (5632, 5632)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|     | 31/72 [00:09<00:13,  3.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (5632, 15104)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|     | 32/72 [00:10<00:14,  2.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (6144, 1152)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|     | 33/72 [00:10<00:12,  3.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (6144, 2048)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|     | 34/72 [00:10<00:11,  3.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (6144, 6144)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|     | 35/72 [00:11<00:11,  3.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (6144, 16384)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|     | 36/72 [00:11<00:14,  2.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (6656, 6656)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|    | 37/72 [00:12<00:13,  2.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (6656, 17920)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|    | 38/72 [00:12<00:15,  2.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (7168, 1280)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|    | 39/72 [00:13<00:13,  2.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (7168, 7168)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|    | 40/72 [00:13<00:12,  2.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (7168, 19200)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|    | 41/72 [00:14<00:15,  2.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (7680, 1408)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|    | 42/72 [00:14<00:12,  2.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (7680, 2560)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|    | 43/72 [00:14<00:11,  2.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (7680, 7680)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|    | 44/72 [00:15<00:10,  2.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (7680, 20480)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|   | 45/72 [00:15<00:14,  1.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (8192, 1536)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|   | 46/72 [00:16<00:11,  2.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (8192, 8192)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|   | 47/72 [00:16<00:10,  2.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (8192, 22016)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|   | 48/72 [00:17<00:14,  1.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (9216, 3072)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|   | 49/72 [00:17<00:11,  1.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (10752, 3584)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|   | 50/72 [00:18<00:10,  2.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (11264, 2048)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|   | 51/72 [00:18<00:08,  2.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (12288, 4096)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|  | 52/72 [00:18<00:07,  2.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (13824, 2560)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|  | 53/72 [00:19<00:07,  2.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (13824, 4608)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|  | 54/72 [00:19<00:06,  2.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (15360, 5120)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|  | 55/72 [00:20<00:07,  2.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (16384, 3072)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|  | 56/72 [00:20<00:06,  2.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (16896, 5632)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|  | 57/72 [00:21<00:06,  2.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (18432, 6144)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|  | 58/72 [00:21<00:06,  2.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (19456, 3584)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%| | 59/72 [00:22<00:06,  2.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (19968, 6656)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%| | 60/72 [00:22<00:06,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (21504, 7168)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%| | 61/72 [00:23<00:07,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (22016, 4096)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%| | 62/72 [00:24<00:06,  1.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (23040, 7680)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%| | 63/72 [00:25<00:06,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (24576, 4608)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%| | 64/72 [00:25<00:05,  1.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (24576, 8192)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%| | 65/72 [00:27<00:05,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (27648, 5120)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|| 66/72 [00:27<00:04,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (30208, 5632)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|| 67/72 [00:28<00:04,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (32768, 6144)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|| 68/72 [00:29<00:03,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (35840, 6656)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|| 69/72 [00:31<00:03,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (38400, 7168)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|| 70/72 [00:32<00:02,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (40960, 7680)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|| 71/72 [00:34<00:01,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking +fp8 (44032, 8192)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 72/72 [00:36<00:00,  1.98it/s]\n"
     ]
    }
   ],
   "source": [
    "run_gpu_benchmarks_layer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quartet vs FP8 speedups:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{102760448: (1.6436503283444779, 0.5548567119298786),\n",
       " 143327232: (1.7420082896924516, 0.5379067805423792),\n",
       " 203161600: (1.8075329850341109, 0.5666629427209339),\n",
       " 265650176: (1.8170496940598866, 0.5782961725734916),\n",
       " 339738624: (1.8191778467781976, 0.6062723140857749),\n",
       " 822083584: (1.942577370260932, 0.7057684643557813),\n",
       " 1585971200: (2.017907839055241, 0.784916852428921),\n",
       " 2717908992: (2.039921327710951, 0.8569070979175183),\n",
       " 4367319040: (2.0518068354058325, 0.933631824900015),\n",
       " 6476005376: (2.1362425133834186, 0.9968699736122014),\n",
       " 9172942848: (2.5086115693167454, 1.0697427769862482),\n",
       " 12687769600: (2.4396833039416137, 1.130157777233169),\n",
       " 16811294720: (2.3057384944943147, 1.1982985526561214),\n",
       " 21743271936: (2.296621562136902, 1.2676162044858852),\n",
       " 27821867008: (2.3000304740218507, 1.2870324411149656),\n",
       " 34630270976: (2.2671545192508957, 1.3216773466176315),\n",
       " 42467328000: (2.2228229682603384, 1.3538777064412824),\n",
       " 51808043008: (2.1351749289424813, 1.3813082313050806)}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speedups = {}\n",
    "for model_size, tensor_shapes in shapes.items():\n",
    "    fp8_forward_latency = 0\n",
    "    fp8_backward_latency = 0\n",
    "    our_forward_latency = 0\n",
    "    our_backward_latency = 0\n",
    "    for key in tensor_shapes:\n",
    "        fp8_forward_latency += RESULTS['+fp8'][key][\"forward_ms\"]\n",
    "        fp8_backward_latency += RESULTS['+fp8'][key][\"total_ms\"] - RESULTS['+fp8'][key][\"forward_ms\"]\n",
    "        our_forward_latency += RESULTS['+cuda'][key][\"forward_ms\"]\n",
    "        our_backward_latency += RESULTS['+cuda'][key][\"total_ms\"] - RESULTS['+cuda'][key][\"forward_ms\"]\n",
    "    speedups[model_size] = (fp8_forward_latency / our_forward_latency, fp8_backward_latency/our_backward_latency)\n",
    "print(\"Quartet vs FP8 speedups:\\n\")\n",
    "speedups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quartet vs BF16 speedups:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{102760448: (3.4634204763853034, 0.8670498751944948),\n",
       " 143327232: (3.6989097957760504, 0.9292802924979252),\n",
       " 203161600: (3.6976511874663722, 0.9884298551374593),\n",
       " 265650176: (3.9742015895950527, 1.1017232827503052),\n",
       " 339738624: (3.954144779816601, 1.1351585694913182),\n",
       " 822083584: (4.3381719026592185, 1.3791725788032718),\n",
       " 1585971200: (4.565072963168387, 1.5764583750728396),\n",
       " 2717908992: (4.688667795765947, 1.7756448723124656),\n",
       " 4367319040: (4.737477547284111, 1.926571275688336),\n",
       " 6476005376: (4.767730387253798, 2.059787790288354),\n",
       " 9172942848: (4.758206326383469, 2.187553268628346),\n",
       " 12687769600: (4.688538115298823, 2.3093788660468935),\n",
       " 16811294720: (4.325882910143264, 2.408245407634389),\n",
       " 21743271936: (4.13393761167307, 2.4869637487860876),\n",
       " 27821867008: (4.0078915143927425, 2.5099089249393907),\n",
       " 34630270976: (3.893967032014315, 2.5997330685382978),\n",
       " 42467328000: (3.810189196247677, 2.639143740663941),\n",
       " 51808043008: (3.6291858200946487, 2.709750179838229)}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speedups = {}\n",
    "for model_size, tensor_shapes in shapes.items():\n",
    "    fp8_forward_latency = 0\n",
    "    fp8_backward_latency = 0\n",
    "    our_forward_latency = 0\n",
    "    our_backward_latency = 0\n",
    "    for key in tensor_shapes:\n",
    "        fp8_forward_latency += RESULTS['baseline'][key][\"forward_ms\"]\n",
    "        fp8_backward_latency += RESULTS['baseline'][key][\"total_ms\"] - RESULTS['baseline'][key][\"forward_ms\"]\n",
    "        our_forward_latency += RESULTS['+cuda'][key][\"forward_ms\"]\n",
    "        our_backward_latency += RESULTS['+cuda'][key][\"total_ms\"] - RESULTS['+cuda'][key][\"forward_ms\"]\n",
    "    speedups[model_size] = (fp8_forward_latency / our_forward_latency, fp8_backward_latency/our_backward_latency)\n",
    "print(\"Quartet vs BF16 speedups:\\n\")\n",
    "speedups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paste them into plots.ipynb!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
